{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf011fc-e30e-4c9b-ba1f-21692ca3355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44d5be-3ed6-4354-aa23-9feea522bf40",
   "metadata": {},
   "source": [
    "# Data split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852681d-107b-4f99-9424-e713ea280d53",
   "metadata": {},
   "source": [
    "## path + calculates how many train data per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99158a-329c-405b-ba95-1da0c03cb0a7",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1510e2-6e07-464d-a71d-95639a59664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data per class [32. 32.  0. 32. 32. 16. 32.  0.]\n",
      "data per test [10. 10.  0. 10. 10.  5. 10.  0.]\n"
     ]
    }
   ],
   "source": [
    "def get_files_from_folder(path):\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    return np.asarray(files)\n",
    "\n",
    "# Path to data\n",
    "path_to_data = \"./data/video/train_ravdess\"\n",
    "# Path to test data where to save\n",
    "path_to_test_data = \"./data/video/test_ravdess\"\n",
    "# Train ratio - 0.7 means splitting data in 70 % train and 30 % test\n",
    "train_ratio = float(0.7)\n",
    "\n",
    "# get dirs\n",
    "_, dirs, _ = next(os.walk(path_to_data))\n",
    "\n",
    "# calculates how many train data per class\n",
    "data_counter_per_class = np.zeros((len(dirs)))\n",
    "for i in range(len(dirs)):\n",
    "    path = os.path.join(path_to_data, dirs[i])\n",
    "    files = get_files_from_folder(path)\n",
    "    data_counter_per_class[i] = len(files)\n",
    "print(\"data per class\", data_counter_per_class)\n",
    "test_counter = np.round(data_counter_per_class * (1 - train_ratio))\n",
    "print(\"data per test\", test_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d13cc-46bd-4478-87dc-6b033ee56e94",
   "metadata": {},
   "source": [
    "## transfers files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5f8b21-a722-4972-829d-bfcd68d687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfers files\n",
    "for i in range(len(dirs)):\n",
    "    path_to_original = os.path.join(path_to_data, dirs[i])\n",
    "    path_to_save = os.path.join(path_to_test_data, dirs[i])\n",
    "\n",
    "    #creates dir\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    files = get_files_from_folder(path_to_original)\n",
    "                                 \n",
    "    # random file sequence\n",
    "    np.random.shuffle(files)\n",
    "                                  \n",
    "    # moves data\n",
    "    for j in range(int(test_counter[i])):\n",
    "        dst = os.path.join(path_to_save, files[j])\n",
    "        src = os.path.join(path_to_original, files[j])\n",
    "        shutil.move(src, dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3567e",
   "metadata": {},
   "source": [
    "# Face alignment part\n",
    "## Preprocess video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e17154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import threading\n",
    "import pdb\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16b8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_threads(threads, n_thread):\n",
    "    used_thread = []\n",
    "    for num, new_thread in enumerate(threads):\n",
    "        print('thread index: {:}'.format(num), end=' \\t')\n",
    "        new_thread.start()\n",
    "        used_thread.append(new_thread)\n",
    "        \n",
    "        if num % n_thread == 0:\n",
    "            for old_thread in used_thread:\n",
    "                old_thread.join()\n",
    "            used_thread = []\n",
    "\n",
    "class threadFun(threading.Thread):\n",
    "    def __init__(self, func, args):\n",
    "        super(threadFun, self).__init__()\n",
    "        self.fun = func\n",
    "        self.args = args\n",
    "    def run(self):\n",
    "        self.fun(*self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_EXTENSIONS = ['mp4', 'webm', 'avi']\n",
    "\n",
    "# check file name, is it end with video type extension\n",
    "def is_video_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in VIDEO_EXTENSIONS)\n",
    "\n",
    "# helper function to make a directory if not exist\n",
    "def makefile(file_dir):\n",
    "    if not os.path.exists(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "\n",
    "# use ffmpeg to extract video to frame to the frame_output\n",
    "def video2frame(video_input, frame_output):\n",
    "    linux_commod = 'ffmpeg -i {:} -f image2 {:}/%07d.jpg'.format(video_input, frame_output)\n",
    "    print('{:}'.format(video_input))\n",
    "    subprocess.getstatusoutput(linux_commod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4784f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vdo2frame(video_dir, frame_dir, n_thread):\n",
    "    print('Starting: convert videos into frames\\nvideo_dir: {:}\\tframe_dir: {:}'.format(video_dir, frame_dir))\n",
    "    threads = []\n",
    "    for root, dirs, files in os.walk(video_dir):\n",
    "        for file_name in files:\n",
    "            if is_video_file(file_name):\n",
    "                # get video name and path\n",
    "                video_name = os.path.join(root, file_name)\n",
    "                # create frame output path from changing video directory in video_name to frame_dir and split the file extension\n",
    "                frame_output_path = os.path.splitext(video_name.replace(video_dir, frame_dir))[0]\n",
    "                makefile(frame_output_path)\n",
    "                threads.append(threadFun(video2frame, (video_name, frame_output_path)))\n",
    "    run_threads(threads, n_thread)\n",
    "    print('all threads is finished') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fb4fa-ae6e-474d-b9cd-6b48b9cf1cb6",
   "metadata": {},
   "source": [
    "### !set path\n",
    "Do on both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adc1a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert videos into frames\n",
      "video_dir: ./data/video/train_ravdess\tframe_dir: ./data/frame/train_ravdess\n",
      "thread index: 0 \t./data/video/train_ravdess\\angry\\01-02-05-01-01-01-04.mp4\n",
      "thread index: 1 \t./data/video/train_ravdess\\angry\\01-02-05-01-01-02-04.mp4\n",
      "thread index: 2 \t./data/video/train_ravdess\\angry\\01-02-05-01-02-01-04.mp4\n",
      "thread index: 3 \t./data/video/train_ravdess\\angry\\01-02-05-01-02-02-04.mp4\n",
      "thread index: 4 \t./data/video/train_ravdess\\angry\\01-02-05-02-02-01-04.mp4\n",
      "thread index: 5 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-01-01.mp4\n",
      "thread index: 6 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-01-02.mp4\n",
      "thread index: 7 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-02-01.mp4\n",
      "thread index: 8 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-02-02.mp4\n",
      "thread index: 9 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-02-03.mp4\n",
      "thread index: 10 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-01-02.mp4\n",
      "thread index: 11 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-01-03.mp4\n",
      "thread index: 12 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-02-01.mp4\n",
      "thread index: 13 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-02-02.mp4\n",
      "thread index: 14 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-02-03.mp4\n",
      "thread index: 15 \t./data/video/train_ravdess\\angry\\02-02-05-02-01-01-02.mp4\n",
      "thread index: 16 \t./data/video/train_ravdess\\angry\\02-02-05-02-01-01-03.mp4\n",
      "thread index: 17 \t./data/video/train_ravdess\\angry\\02-02-05-02-01-02-01.mp4\n",
      "thread index: 18 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-01-03.mp4\n",
      "thread index: 19 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-02-01.mp4\n",
      "thread index: 20 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-02-02.mp4\n",
      "thread index: 21 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-02-03.mp4\n",
      "thread index: 22 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-01-01.mp4\n",
      "thread index: 23 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-01-02.mp4\n",
      "thread index: 24 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-02-01.mp4\n",
      "thread index: 25 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-02-02.mp4\n",
      "thread index: 26 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-02-04.mp4\n",
      "thread index: 27 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-01-01.mp4\n",
      "thread index: 28 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-01-02.mp4\n",
      "thread index: 29 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-01-03.mp4\n",
      "thread index: 30 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-02-01.mp4\n",
      "thread index: 31 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-02-02.mp4\n",
      "thread index: 32 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-02-03.mp4\n",
      "thread index: 33 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-01-02.mp4\n",
      "thread index: 34 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-01-03.mp4\n",
      "thread index: 35 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-01-04.mp4\n",
      "thread index: 36 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-02-01.mp4\n",
      "thread index: 37 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-02-02.mp4\n",
      "thread index: 38 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-02-04.mp4\n",
      "thread index: 39 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-01-02.mp4\n",
      "thread index: 40 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-01-03.mp4\n",
      "thread index: 41 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-01-04.mp4\n",
      "thread index: 42 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-02-01.mp4\n",
      "thread index: 43 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-02-04.mp4\n",
      "thread index: 44 \t./data/video/train_ravdess\\fearful\\01-02-06-01-01-01-04.mp4\n",
      "thread index: 45 \t./data/video/train_ravdess\\fearful\\01-02-06-01-01-02-04.mp4\n",
      "thread index: 46 \t./data/video/train_ravdess\\fearful\\01-02-06-01-02-02-04.mp4\n",
      "thread index: 47 \t./data/video/train_ravdess\\fearful\\01-02-06-02-02-01-04.mp4\n",
      "thread index: 48 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-01-02.mp4\n",
      "thread index: 49 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-01-03.mp4\n",
      "thread index: 50 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-02-01.mp4\n",
      "thread index: 51 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-02-03.mp4\n",
      "thread index: 52 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-01-01.mp4\n",
      "thread index: 53 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-01-02.mp4\n",
      "thread index: 54 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-01-03.mp4\n",
      "thread index: 55 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-02-01.mp4\n",
      "thread index: 56 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-02-03.mp4\n",
      "thread index: 57 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-01-01.mp4\n",
      "thread index: 58 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-02-01.mp4\n",
      "thread index: 59 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-02-02.mp4\n",
      "thread index: 60 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-02-03.mp4\n",
      "thread index: 61 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-01-02.mp4\n",
      "thread index: 62 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-01-03.mp4\n",
      "thread index: 63 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-02-01.mp4\n",
      "thread index: 64 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-02-02.mp4\n",
      "thread index: 65 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-02-03.mp4\n",
      "thread index: 66 \t./data/video/train_ravdess\\happy\\01-02-03-01-01-01-04.mp4\n",
      "thread index: 67 \t./data/video/train_ravdess\\happy\\01-02-03-01-01-02-04.mp4\n",
      "thread index: 68 \t./data/video/train_ravdess\\happy\\01-02-03-02-01-01-04.mp4\n",
      "thread index: 69 \t./data/video/train_ravdess\\happy\\01-02-03-02-01-02-04.mp4\n",
      "thread index: 70 \t./data/video/train_ravdess\\happy\\01-02-03-02-02-01-04.mp4\n",
      "thread index: 71 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-01-02.mp4\n",
      "thread index: 72 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-01-03.mp4\n",
      "thread index: 73 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-02-01.mp4\n",
      "thread index: 74 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-02-02.mp4\n",
      "thread index: 75 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-02-03.mp4\n",
      "thread index: 76 \t./data/video/train_ravdess\\happy\\02-02-03-01-02-01-01.mp4\n",
      "thread index: 77 \t./data/video/train_ravdess\\happy\\02-02-03-01-02-01-02.mp4\n",
      "thread index: 78 \t./data/video/train_ravdess\\happy\\02-02-03-01-02-02-03.mp4\n",
      "thread index: 79 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-01-01.mp4\n",
      "thread index: 80 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-01-02.mp4\n",
      "thread index: 81 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-01-03.mp4\n",
      "thread index: 82 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-02-02.mp4\n",
      "thread index: 83 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-02-03.mp4\n",
      "thread index: 84 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-01-03.mp4\n",
      "thread index: 85 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-02-01.mp4\n",
      "thread index: 86 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-02-02.mp4\n",
      "thread index: 87 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-02-03.mp4\n",
      "thread index: 88 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-01-01.mp4\n",
      "thread index: 89 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-01-03.mp4\n",
      "thread index: 90 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-02-01.mp4\n",
      "thread index: 91 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-02-02.mp4\n",
      "thread index: 92 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-02-04.mp4\n",
      "thread index: 93 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-01-02.mp4\n",
      "thread index: 94 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-01-03.mp4\n",
      "thread index: 95 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-01-04.mp4\n",
      "thread index: 96 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-02-02.mp4\n",
      "thread index: 97 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-02-03.mp4\n",
      "thread index: 98 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-02-04.mp4\n",
      "thread index: 99 \t./data/video/train_ravdess\\sad\\01-02-04-01-01-01-04.mp4\n",
      "thread index: 100 \t./data/video/train_ravdess\\sad\\01-02-04-01-01-02-04.mp4\n",
      "thread index: 101 \t./data/video/train_ravdess\\sad\\01-02-04-01-02-02-04.mp4\n",
      "thread index: 102 \t./data/video/train_ravdess\\sad\\01-02-04-02-01-01-04.mp4\n",
      "thread index: 103 \t./data/video/train_ravdess\\sad\\01-02-04-02-01-02-04.mp4\n",
      "thread index: 104 \t./data/video/train_ravdess\\sad\\01-02-04-02-02-02-04.mp4\n",
      "thread index: 105 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-01-01.mp4\n",
      "thread index: 106 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-01-02.mp4\n",
      "thread index: 107 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-01-03.mp4\n",
      "thread index: 108 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-02-01.mp4\n",
      "thread index: 109 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-02-02.mp4\n",
      "thread index: 110 \t./data/video/train_ravdess\\sad\\02-02-04-01-02-01-02.mp4\n",
      "thread index: 111 \t./data/video/train_ravdess\\sad\\02-02-04-01-02-01-03.mp4\n",
      "thread index: 112 \t./data/video/train_ravdess\\sad\\02-02-04-01-02-02-01.mp4\n",
      "thread index: 113 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-01-01.mp4\n",
      "thread index: 114 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-01-03.mp4\n",
      "thread index: 115 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-02-02.mp4\n",
      "thread index: 116 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-02-03.mp4\n",
      "thread index: 117 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-01-01.mp4\n",
      "thread index: 118 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-01-03.mp4\n",
      "thread index: 119 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-02-01.mp4\n",
      "thread index: 120 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-02-02.mp4\n",
      "all threads is finished\n"
     ]
    }
   ],
   "source": [
    "video_dir_train ='./data/video/train_ravdess'\n",
    "frame_dir_train = './data/frame/train_ravdess'\n",
    "run_vdo2frame(video_dir_train, frame_dir_train, n_thread =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec702d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert videos into frames\n",
      "video_dir: ./data/video/test_ravdess\tframe_dir: ./data/frame/test_ravdess\n",
      "thread index: 0 \t./data/video/test_ravdess\\angry\\01-02-05-02-01-01-04.mp4\n",
      "thread index: 1 \t./data/video/test_ravdess\\angry\\01-02-05-02-01-02-04.mp4\n",
      "thread index: 2 \t./data/video/test_ravdess\\angry\\01-02-05-02-02-02-04.mp4\n",
      "thread index: 3 \t./data/video/test_ravdess\\angry\\02-02-05-01-01-01-03.mp4\n",
      "thread index: 4 \t./data/video/test_ravdess\\angry\\02-02-05-01-02-01-01.mp4\n",
      "thread index: 5 \t./data/video/test_ravdess\\angry\\02-02-05-02-01-01-01.mp4\n",
      "thread index: 6 \t./data/video/test_ravdess\\angry\\02-02-05-02-01-02-02.mp4\n",
      "thread index: 7 \t./data/video/test_ravdess\\angry\\02-02-05-02-01-02-03.mp4\n",
      "thread index: 8 \t./data/video/test_ravdess\\angry\\02-02-05-02-02-01-01.mp4\n",
      "thread index: 9 \t./data/video/test_ravdess\\angry\\02-02-05-02-02-01-02.mp4\n",
      "thread index: 10 \t./data/video/test_ravdess\\calm\\02-02-02-01-01-01-03.mp4\n",
      "thread index: 11 \t./data/video/test_ravdess\\calm\\02-02-02-01-01-01-04.mp4\n",
      "thread index: 12 \t./data/video/test_ravdess\\calm\\02-02-02-01-01-02-03.mp4\n",
      "thread index: 13 \t./data/video/test_ravdess\\calm\\02-02-02-01-02-01-04.mp4\n",
      "thread index: 14 \t./data/video/test_ravdess\\calm\\02-02-02-01-02-02-04.mp4\n",
      "thread index: 15 \t./data/video/test_ravdess\\calm\\02-02-02-02-01-01-01.mp4\n",
      "thread index: 16 \t./data/video/test_ravdess\\calm\\02-02-02-02-01-02-03.mp4\n",
      "thread index: 17 \t./data/video/test_ravdess\\calm\\02-02-02-02-02-01-01.mp4\n",
      "thread index: 18 \t./data/video/test_ravdess\\calm\\02-02-02-02-02-02-02.mp4\n",
      "thread index: 19 \t./data/video/test_ravdess\\calm\\02-02-02-02-02-02-03.mp4\n",
      "thread index: 20 \t./data/video/test_ravdess\\fearful\\01-02-06-01-02-01-04.mp4\n",
      "thread index: 21 \t./data/video/test_ravdess\\fearful\\01-02-06-02-01-01-04.mp4\n",
      "thread index: 22 \t./data/video/test_ravdess\\fearful\\01-02-06-02-01-02-04.mp4\n",
      "thread index: 23 \t./data/video/test_ravdess\\fearful\\01-02-06-02-02-02-04.mp4\n",
      "thread index: 24 \t./data/video/test_ravdess\\fearful\\02-02-06-01-01-01-01.mp4\n",
      "thread index: 25 \t./data/video/test_ravdess\\fearful\\02-02-06-01-01-02-02.mp4\n",
      "thread index: 26 \t./data/video/test_ravdess\\fearful\\02-02-06-01-02-02-02.mp4\n",
      "thread index: 27 \t./data/video/test_ravdess\\fearful\\02-02-06-02-01-01-02.mp4\n",
      "thread index: 28 \t./data/video/test_ravdess\\fearful\\02-02-06-02-01-01-03.mp4\n",
      "thread index: 29 \t./data/video/test_ravdess\\fearful\\02-02-06-02-02-01-01.mp4\n",
      "thread index: 30 \t./data/video/test_ravdess\\happy\\01-02-03-01-02-01-04.mp4\n",
      "thread index: 31 \t./data/video/test_ravdess\\happy\\01-02-03-01-02-02-04.mp4\n",
      "thread index: 32 \t./data/video/test_ravdess\\happy\\01-02-03-02-02-02-04.mp4\n",
      "thread index: 33 \t./data/video/test_ravdess\\happy\\02-02-03-01-01-01-01.mp4\n",
      "thread index: 34 \t./data/video/test_ravdess\\happy\\02-02-03-01-02-01-03.mp4\n",
      "thread index: 35 \t./data/video/test_ravdess\\happy\\02-02-03-01-02-02-01.mp4\n",
      "thread index: 36 \t./data/video/test_ravdess\\happy\\02-02-03-01-02-02-02.mp4\n",
      "thread index: 37 \t./data/video/test_ravdess\\happy\\02-02-03-02-01-02-01.mp4\n",
      "thread index: 38 \t./data/video/test_ravdess\\happy\\02-02-03-02-02-01-01.mp4\n",
      "thread index: 39 \t./data/video/test_ravdess\\happy\\02-02-03-02-02-01-02.mp4\n",
      "thread index: 40 \t./data/video/test_ravdess\\neutral\\02-02-01-01-01-01-02.mp4\n",
      "thread index: 41 \t./data/video/test_ravdess\\neutral\\02-02-01-01-01-01-04.mp4\n",
      "thread index: 42 \t./data/video/test_ravdess\\neutral\\02-02-01-01-01-02-03.mp4\n",
      "thread index: 43 \t./data/video/test_ravdess\\neutral\\02-02-01-01-02-01-01.mp4\n",
      "thread index: 44 \t./data/video/test_ravdess\\neutral\\02-02-01-01-02-02-01.mp4\n",
      "thread index: 45 \t./data/video/test_ravdess\\sad\\01-02-04-01-02-01-04.mp4\n",
      "thread index: 46 \t./data/video/test_ravdess\\sad\\01-02-04-02-02-01-04.mp4\n",
      "thread index: 47 \t./data/video/test_ravdess\\sad\\02-02-04-01-01-02-03.mp4\n",
      "thread index: 48 \t./data/video/test_ravdess\\sad\\02-02-04-01-02-01-01.mp4\n",
      "thread index: 49 \t./data/video/test_ravdess\\sad\\02-02-04-01-02-02-02.mp4\n",
      "thread index: 50 \t./data/video/test_ravdess\\sad\\02-02-04-01-02-02-03.mp4\n",
      "thread index: 51 \t./data/video/test_ravdess\\sad\\02-02-04-02-01-01-02.mp4\n",
      "thread index: 52 \t./data/video/test_ravdess\\sad\\02-02-04-02-01-02-01.mp4\n",
      "thread index: 53 \t./data/video/test_ravdess\\sad\\02-02-04-02-02-01-02.mp4\n",
      "thread index: 54 \t./data/video/test_ravdess\\sad\\02-02-04-02-02-02-03.mp4\n",
      "all threads is finished\n"
     ]
    }
   ],
   "source": [
    "video_dir_test ='./data/video/test_ravdess'\n",
    "frame_dir_test = './data/frame/test_ravdess'\n",
    "run_vdo2frame(video_dir_test, frame_dir_test, n_thread = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6b7d",
   "metadata": {},
   "source": [
    "## Face localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e5ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame2face(func_path, predictor_path, image_root_folder, save_root_folder, cnn_face_detector, gpu_id=0):\n",
    "\n",
    "    linux_command = 'python {:} {:} {:} {:} {:} {:}'.format(func_path, predictor_path, \n",
    "                                                            image_root_folder, save_root_folder, cnn_face_detector, gpu_id)\n",
    "    print('{:}'.format(image_root_folder))\n",
    "    subprocess.getstatusoutput(linux_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76dd7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_frame2face(frame_dir, face_dir, n_thread):\n",
    "    threads = []\n",
    "    # function\n",
    "    func_path = './data/face_alignment_code/lib/face_align_cuda.py'\n",
    "    # Model\n",
    "    predictor_path      = './data/face_alignment_code/lib/shape_predictor_5_face_landmarks.dat'\n",
    "    cnn_face_detector   = './data/face_alignment_code/lib/mmod_human_face_detector.dat' \n",
    "    for category in os.listdir(frame_dir):\n",
    "        # create category directory\n",
    "        category_dir = os.path.join(frame_dir, category)\n",
    "        for frame_file in os.listdir(category_dir):\n",
    "            # get frame file name and path for each frame\n",
    "            frame_root_folder = os.path.join(category_dir, frame_file)\n",
    "            # create output path for face localization\n",
    "            face_root_folder = frame_root_folder.replace(frame_dir, face_dir)\n",
    "            if os.path.isdir(frame_root_folder):\n",
    "                makefile(face_root_folder)\n",
    "                threads.append(threadFun(frame2face, (func_path, predictor_path, frame_root_folder, face_root_folder, cnn_face_detector)))\n",
    "\n",
    "    run_threads(threads, n_thread)\n",
    "    print('all is over')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2fddc-94c8-4208-8783-34bcb1347500",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d498f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread index: 0 \t./data/frame/train_ravdess\\angry\\01-02-05-01-01-01-04\n",
      "thread index: 1 \t./data/frame/train_ravdess\\angry\\01-02-05-01-01-02-04\n",
      "thread index: 2 \t./data/frame/train_ravdess\\angry\\01-02-05-01-02-01-04\n",
      "thread index: 3 \t./data/frame/train_ravdess\\angry\\01-02-05-01-02-02-04\n",
      "thread index: 4 \t./data/frame/train_ravdess\\angry\\01-02-05-02-02-01-04\n",
      "thread index: 5 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-01-01\n",
      "thread index: 6 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-01-02\n",
      "thread index: 7 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-02-01\n",
      "thread index: 8 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-02-02\n",
      "thread index: 9 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-02-03\n",
      "thread index: 10 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-01-02\n",
      "thread index: 11 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-01-03\n",
      "thread index: 12 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-02-01\n",
      "thread index: 13 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-02-02\n",
      "thread index: 14 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-02-03\n",
      "thread index: 15 \t./data/frame/train_ravdess\\angry\\02-02-05-02-01-01-02\n",
      "thread index: 16 \t./data/frame/train_ravdess\\angry\\02-02-05-02-01-01-03\n",
      "thread index: 17 \t./data/frame/train_ravdess\\angry\\02-02-05-02-01-02-01\n",
      "thread index: 18 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-01-03\n",
      "thread index: 19 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-02-01\n",
      "thread index: 20 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-02-02\n",
      "thread index: 21 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-02-03\n",
      "thread index: 22 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-01-01\n",
      "thread index: 23 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-01-02\n",
      "thread index: 24 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-02-01\n",
      "thread index: 25 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-02-02\n",
      "thread index: 26 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-02-04\n",
      "thread index: 27 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-01-01\n",
      "thread index: 28 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-01-02\n",
      "thread index: 29 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-01-03\n",
      "thread index: 30 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-02-01\n",
      "thread index: 31 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-02-02\n",
      "thread index: 32 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-02-03\n",
      "thread index: 33 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-01-02\n",
      "thread index: 34 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-01-03\n",
      "thread index: 35 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-01-04\n",
      "thread index: 36 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-02-01\n",
      "thread index: 37 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-02-02\n",
      "thread index: 38 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-02-04\n",
      "thread index: 39 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-01-02\n",
      "thread index: 40 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-01-03\n",
      "thread index: 41 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-01-04\n",
      "thread index: 42 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-02-01\n",
      "thread index: 43 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-02-04\n",
      "thread index: 44 \t./data/frame/train_ravdess\\fearful\\01-02-06-01-01-01-04\n",
      "thread index: 45 \t./data/frame/train_ravdess\\fearful\\01-02-06-01-01-02-04\n",
      "thread index: 46 \t./data/frame/train_ravdess\\fearful\\01-02-06-01-02-02-04\n",
      "thread index: 47 \t./data/frame/train_ravdess\\fearful\\01-02-06-02-02-01-04\n",
      "thread index: 48 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-01-02\n",
      "thread index: 49 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-01-03\n",
      "thread index: 50 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-02-01\n",
      "thread index: 51 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-02-03\n",
      "thread index: 52 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-01-01\n",
      "thread index: 53 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-01-02\n",
      "thread index: 54 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-01-03\n",
      "thread index: 55 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-02-01\n",
      "thread index: 56 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-02-03\n",
      "thread index: 57 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-01-01\n",
      "thread index: 58 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-02-01\n",
      "thread index: 59 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-02-02\n",
      "thread index: 60 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-02-03\n",
      "thread index: 61 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-01-02\n",
      "thread index: 62 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-01-03\n",
      "thread index: 63 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-02-01\n",
      "thread index: 64 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-02-02\n",
      "thread index: 65 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-02-03\n",
      "thread index: 66 \t./data/frame/train_ravdess\\happy\\01-02-03-01-01-01-04\n",
      "thread index: 67 \t./data/frame/train_ravdess\\happy\\01-02-03-01-01-02-04\n",
      "thread index: 68 \t./data/frame/train_ravdess\\happy\\01-02-03-02-01-01-04\n",
      "thread index: 69 \t./data/frame/train_ravdess\\happy\\01-02-03-02-01-02-04\n",
      "thread index: 70 \t./data/frame/train_ravdess\\happy\\01-02-03-02-02-01-04\n",
      "thread index: 71 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-01-02\n",
      "thread index: 72 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-01-03\n",
      "thread index: 73 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-02-01\n",
      "thread index: 74 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-02-02\n",
      "thread index: 75 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-02-03\n",
      "thread index: 76 \t./data/frame/train_ravdess\\happy\\02-02-03-01-02-01-01\n",
      "thread index: 77 \t./data/frame/train_ravdess\\happy\\02-02-03-01-02-01-02\n",
      "thread index: 78 \t./data/frame/train_ravdess\\happy\\02-02-03-01-02-02-03\n",
      "thread index: 79 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-01-01\n",
      "thread index: 80 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-01-02\n",
      "thread index: 81 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-01-03\n",
      "thread index: 82 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-02-02\n",
      "thread index: 83 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-02-03\n",
      "thread index: 84 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-01-03\n",
      "thread index: 85 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-02-01\n",
      "thread index: 86 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-02-02\n",
      "thread index: 87 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-02-03\n",
      "thread index: 88 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-01-01\n",
      "thread index: 89 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-01-03\n",
      "thread index: 90 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-02-01\n",
      "thread index: 91 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-02-02\n",
      "thread index: 92 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-02-04\n",
      "thread index: 93 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-01-02\n",
      "thread index: 94 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-01-03\n",
      "thread index: 95 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-01-04\n",
      "thread index: 96 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-02-02\n",
      "thread index: 97 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-02-03\n",
      "thread index: 98 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-02-04\n",
      "thread index: 99 \t./data/frame/train_ravdess\\sad\\01-02-04-01-01-01-04\n",
      "thread index: 100 \t./data/frame/train_ravdess\\sad\\01-02-04-01-01-02-04\n",
      "thread index: 101 \t./data/frame/train_ravdess\\sad\\01-02-04-01-02-02-04\n",
      "thread index: 102 \t./data/frame/train_ravdess\\sad\\01-02-04-02-01-01-04\n",
      "thread index: 103 \t./data/frame/train_ravdess\\sad\\01-02-04-02-01-02-04\n",
      "thread index: 104 \t./data/frame/train_ravdess\\sad\\01-02-04-02-02-02-04\n",
      "thread index: 105 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-01-01\n",
      "thread index: 106 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-01-02\n",
      "thread index: 107 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-01-03\n",
      "thread index: 108 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-02-01\n",
      "thread index: 109 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-02-02\n",
      "thread index: 110 \t./data/frame/train_ravdess\\sad\\02-02-04-01-02-01-02\n",
      "thread index: 111 \t./data/frame/train_ravdess\\sad\\02-02-04-01-02-01-03\n",
      "thread index: 112 \t./data/frame/train_ravdess\\sad\\02-02-04-01-02-02-01\n",
      "thread index: 113 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-01-01\n",
      "thread index: 114 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-01-03\n",
      "thread index: 115 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-02-02\n",
      "thread index: 116 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-02-03\n",
      "thread index: 117 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-01-01\n",
      "thread index: 118 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-01-03\n",
      "thread index: 119 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-02-01\n",
      "thread index: 120 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-02-02\n",
      "all is over\n"
     ]
    }
   ],
   "source": [
    "frame_dir_train = './data/frame/train_ravdess'\n",
    "face_dir_train  = './data/face/train_ravdess'\n",
    "run_frame2face(frame_dir_train, face_dir_train, n_thread=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afeb268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread index: 0 \t./data/frame/test_ravdess\\angry\\01-02-05-02-01-01-04\n",
      "thread index: 1 \t./data/frame/test_ravdess\\angry\\01-02-05-02-01-02-04\n",
      "thread index: 2 \t./data/frame/test_ravdess\\angry\\01-02-05-02-02-02-04\n",
      "thread index: 3 \t./data/frame/test_ravdess\\angry\\02-02-05-01-01-01-03\n",
      "thread index: 4 \t./data/frame/test_ravdess\\angry\\02-02-05-01-02-01-01\n",
      "thread index: 5 \t./data/frame/test_ravdess\\angry\\02-02-05-02-01-01-01\n",
      "thread index: 6 \t./data/frame/test_ravdess\\angry\\02-02-05-02-01-02-02\n",
      "thread index: 7 \t./data/frame/test_ravdess\\angry\\02-02-05-02-01-02-03\n",
      "thread index: 8 \t./data/frame/test_ravdess\\angry\\02-02-05-02-02-01-01\n",
      "thread index: 9 \t./data/frame/test_ravdess\\angry\\02-02-05-02-02-01-02\n",
      "thread index: 10 \t./data/frame/test_ravdess\\calm\\02-02-02-01-01-01-03\n",
      "thread index: 11 \t./data/frame/test_ravdess\\calm\\02-02-02-01-01-01-04\n",
      "thread index: 12 \t./data/frame/test_ravdess\\calm\\02-02-02-01-01-02-03\n",
      "thread index: 13 \t./data/frame/test_ravdess\\calm\\02-02-02-01-02-01-04\n",
      "thread index: 14 \t./data/frame/test_ravdess\\calm\\02-02-02-01-02-02-04\n",
      "thread index: 15 \t./data/frame/test_ravdess\\calm\\02-02-02-02-01-01-01\n",
      "thread index: 16 \t./data/frame/test_ravdess\\calm\\02-02-02-02-01-02-03\n",
      "thread index: 17 \t./data/frame/test_ravdess\\calm\\02-02-02-02-02-01-01\n",
      "thread index: 18 \t./data/frame/test_ravdess\\calm\\02-02-02-02-02-02-02\n",
      "thread index: 19 \t./data/frame/test_ravdess\\calm\\02-02-02-02-02-02-03\n",
      "thread index: 20 \t./data/frame/test_ravdess\\fearful\\01-02-06-01-02-01-04\n",
      "thread index: 21 \t./data/frame/test_ravdess\\fearful\\01-02-06-02-01-01-04\n",
      "thread index: 22 \t./data/frame/test_ravdess\\fearful\\01-02-06-02-01-02-04\n",
      "thread index: 23 \t./data/frame/test_ravdess\\fearful\\01-02-06-02-02-02-04\n",
      "thread index: 24 \t./data/frame/test_ravdess\\fearful\\02-02-06-01-01-01-01\n",
      "thread index: 25 \t./data/frame/test_ravdess\\fearful\\02-02-06-01-01-02-02\n",
      "thread index: 26 \t./data/frame/test_ravdess\\fearful\\02-02-06-01-02-02-02\n",
      "thread index: 27 \t./data/frame/test_ravdess\\fearful\\02-02-06-02-01-01-02\n",
      "thread index: 28 \t./data/frame/test_ravdess\\fearful\\02-02-06-02-01-01-03\n",
      "thread index: 29 \t./data/frame/test_ravdess\\fearful\\02-02-06-02-02-01-01\n",
      "thread index: 30 \t./data/frame/test_ravdess\\happy\\01-02-03-01-02-01-04\n",
      "thread index: 31 \t./data/frame/test_ravdess\\happy\\01-02-03-01-02-02-04\n",
      "thread index: 32 \t./data/frame/test_ravdess\\happy\\01-02-03-02-02-02-04\n",
      "thread index: 33 \t./data/frame/test_ravdess\\happy\\02-02-03-01-01-01-01\n",
      "thread index: 34 \t./data/frame/test_ravdess\\happy\\02-02-03-01-02-01-03\n",
      "thread index: 35 \t./data/frame/test_ravdess\\happy\\02-02-03-01-02-02-01\n",
      "thread index: 36 \t./data/frame/test_ravdess\\happy\\02-02-03-01-02-02-02\n",
      "thread index: 37 \t./data/frame/test_ravdess\\happy\\02-02-03-02-01-02-01\n",
      "thread index: 38 \t./data/frame/test_ravdess\\happy\\02-02-03-02-02-01-01\n",
      "thread index: 39 \t./data/frame/test_ravdess\\happy\\02-02-03-02-02-01-02\n",
      "thread index: 40 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-01-01-02\n",
      "thread index: 41 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-01-01-04\n",
      "thread index: 42 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-01-02-03\n",
      "thread index: 43 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-02-01-01\n",
      "thread index: 44 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-02-02-01\n",
      "thread index: 45 \t./data/frame/test_ravdess\\sad\\01-02-04-01-02-01-04\n",
      "thread index: 46 \t./data/frame/test_ravdess\\sad\\01-02-04-02-02-01-04\n",
      "thread index: 47 \t./data/frame/test_ravdess\\sad\\02-02-04-01-01-02-03\n",
      "thread index: 48 \t./data/frame/test_ravdess\\sad\\02-02-04-01-02-01-01\n",
      "thread index: 49 \t./data/frame/test_ravdess\\sad\\02-02-04-01-02-02-02\n",
      "thread index: 50 \t./data/frame/test_ravdess\\sad\\02-02-04-01-02-02-03\n",
      "thread index: 51 \t./data/frame/test_ravdess\\sad\\02-02-04-02-01-01-02\n",
      "thread index: 52 \t./data/frame/test_ravdess\\sad\\02-02-04-02-01-02-01\n",
      "thread index: 53 \t./data/frame/test_ravdess\\sad\\02-02-04-02-02-01-02\n",
      "thread index: 54 \t./data/frame/test_ravdess\\sad\\02-02-04-02-02-02-03\n",
      "all is over\n"
     ]
    }
   ],
   "source": [
    "frame_dir_test = './data/frame/test_ravdess' \n",
    "face_dir_test  = './data/face/test_ravdess'\n",
    "run_frame2face(frame_dir_test, face_dir_test, n_thread=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f06db",
   "metadata": {},
   "source": [
    "## Create text file for train list and label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff1987",
   "metadata": {},
   "source": [
    " Example Tree-Structured Data Directory (same as test data) <br> \n",
    "── train_data <br>\n",
    "   ├── Angry <br>\n",
    "   ├── Disgust <br>\n",
    "   ├── Fear <br>\n",
    "   ├── Happy <br>\n",
    "   ├── Neutral <br>\n",
    "   ├── Sad <br>\n",
    "   └── Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1535d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt(dir_path, output_path):\n",
    "    # Open the output file in write mode\n",
    "    with open(output_path, \"w\") as f:\n",
    "    # Walk through the directory\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            # Skip if no files in the directory\n",
    "            if not files:\n",
    "                continue\n",
    "            # Get the parent directory name as the label\n",
    "            label = os.path.basename(os.path.dirname(root))\n",
    "            # Write the directory path (relative to dir_path) and label to the output file\n",
    "            relative_path = os.path.relpath(root, dir_path)\n",
    "            relative_path = relative_path.replace(\"\\\\\", \"/\")\n",
    "            f.write(f\"{relative_path} {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f97a4-cb15-4030-8492-905ff95175fe",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cb95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "# Define the directory to scan\n",
    "train_dir_path = \"./data/face/train_ravdess\"\n",
    "\n",
    "# Define the output file path\n",
    "train_output_file_path = \"./data/txt/train_ravdess.txt\"\n",
    "\n",
    "get_txt(train_dir_path, train_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a5205-6196-426a-a0d7-530880363e29",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926b6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# Define the directory to scan\n",
    "test_dir_path = \"./data/face/test_ravdess\"\n",
    "\n",
    "# Define the output file path\n",
    "test_output_file_path = \"./data/txt/test_ravdess.txt\"\n",
    "\n",
    "get_txt(test_dir_path, test_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ae545",
   "metadata": {},
   "source": [
    "# Modeling !!Before Start all process (run from here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648e9bd",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76d71d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2836aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os, sys, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee7b3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data generator for afew\n",
    "class VideoDataset(data.Dataset):\n",
    "    def __init__(self, video_root, video_list, rectify_label=None, transform=None, csv = False):\n",
    "\n",
    "        self.imgs_first, self.index = load_imgs_total_frame(video_root, video_list, rectify_label)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        return img_first, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "# \n",
    "class TripleImageDataset(data.Dataset):\n",
    "    def __init__(self, video_root, video_list, rectify_label=None, transform=None):\n",
    "\n",
    "        self.imgs_first, self.imgs_second, self.imgs_third, self.index = load_imgs_tsn(video_root, video_list,\n",
    "                                                                                           rectify_label)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        path_second, target_second = self.imgs_second[index]\n",
    "        img_second = Image.open(path_second).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_second = self.transform(img_second)\n",
    "\n",
    "        path_third, target_third = self.imgs_third[index]\n",
    "        img_third = Image.open(path_third).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_third = self.transform(img_third)\n",
    "        return img_first, img_second, img_third, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "def load_imgs_tsn(video_root, video_list, rectify_label):\n",
    "    imgs_first = list()\n",
    "    imgs_second = list()\n",
    "    imgs_third = list()\n",
    "\n",
    "    with open(video_list, 'r') as imf:\n",
    "        index = []\n",
    "        for id, line in enumerate(imf):\n",
    "\n",
    "            video_label = line.strip().split()\n",
    "\n",
    "            video_name = video_label[0]  # name of video\n",
    "            label = rectify_label[video_label[1]]  # label of video\n",
    "\n",
    "            video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "            ###  for sampling triple imgs in the single video_path  ####\n",
    "\n",
    "            img_lists = os.listdir(video_path)\n",
    "            img_lists.sort()  # sort files by ascending\n",
    "            img_count = len(img_lists)  # number of frames in video\n",
    "            num_per_part = int(img_count) // 3\n",
    "\n",
    "            if int(img_count) > 3:\n",
    "                for i in range(img_count):\n",
    "\n",
    "                    random_select_first = random.randint(0, num_per_part)\n",
    "                    random_select_second = random.randint(num_per_part, num_per_part * 2)\n",
    "                    random_select_third = random.randint(2 * num_per_part, len(img_lists) - 1)\n",
    "\n",
    "                    img_path_first = os.path.join(video_path, img_lists[random_select_first])\n",
    "                    img_path_second = os.path.join(video_path, img_lists[random_select_second])\n",
    "                    img_path_third = os.path.join(video_path, img_lists[random_select_third])\n",
    "\n",
    "                    imgs_first.append((img_path_first, label))\n",
    "                    imgs_second.append((img_path_second, label))\n",
    "                    imgs_third.append((img_path_third, label))\n",
    "\n",
    "            else:\n",
    "                for j in range(len(img_lists)):\n",
    "                    img_path_first = os.path.join(video_path, img_lists[j])\n",
    "                    img_path_second = os.path.join(video_path, random.choice(img_lists))\n",
    "                    img_path_third = os.path.join(video_path, random.choice(img_lists))\n",
    "\n",
    "                    imgs_first.append((img_path_first, label))\n",
    "                    imgs_second.append((img_path_second, label))\n",
    "                    imgs_third.append((img_path_third, label))\n",
    "\n",
    "            ###  return video frame index  #####\n",
    "            index.append(np.ones(img_count) * id)  # id: 0 : 379\n",
    "        index = np.concatenate(index, axis=0)\n",
    "        # index = index.astype(int)\n",
    "    return imgs_first, imgs_second, imgs_third, index\n",
    "\n",
    "\n",
    "def load_imgs_total_frame(video_root, video_list, rectify_label):\n",
    "    imgs_first = list()\n",
    "\n",
    "    with open(video_list, 'r') as imf:\n",
    "        index = []\n",
    "        video_names = []\n",
    "        for id, line in enumerate(imf):\n",
    "\n",
    "            video_label = line.strip().split()\n",
    "\n",
    "            video_name = video_label[0]  # name of video\n",
    "            label = rectify_label[video_label[1]]  # label of video\n",
    "\n",
    "            video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "            ###  for sampling triple imgs in the single video_path  ####\n",
    "\n",
    "            img_lists = os.listdir(video_path)\n",
    "            img_lists.sort()  # sort files by ascending\n",
    "            img_count = len(img_lists)  # number of frames in video\n",
    "\n",
    "            for frame in img_lists:\n",
    "                # pdb.set_trace()\n",
    "                imgs_first.append((os.path.join(video_path, frame), label))\n",
    "            ###  return video frame index  #####\n",
    "            video_names.append(video_name)\n",
    "            index.append(np.ones(img_count) * id)\n",
    "        index = np.concatenate(index, axis=0)\n",
    "        # index = index.astype(int)\n",
    "    return imgs_first, index\n",
    "    \n",
    "## data generator for ck_plus\n",
    "class TenFold_VideoDataset(data.Dataset):\n",
    "    def __init__(self, video_root='', video_list='', rectify_label=None, transform=None, fold=1, run_type='train'):\n",
    "        self.imgs_first, self.index = load_imgs_tenfold_totalframe(video_root, video_list, rectify_label, fold, run_type)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        return img_first, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "class TenFold_TripleImageDataset(data.Dataset):\n",
    "    def __init__(self, video_root='', video_list='', rectify_label=None, transform=None, fold=1, run_type='train'):\n",
    "\n",
    "        self.imgs_first, self.imgs_second, self.imgs_third, self.index = load_imgs_tsn_tenfold(video_root,video_list,rectify_label, fold, run_type)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        path_second, target_second = self.imgs_second[index]\n",
    "        img_second = Image.open(path_second).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_second = self.transform(img_second)\n",
    "\n",
    "        path_third, target_third = self.imgs_third[index]\n",
    "        img_third = Image.open(path_third).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_third = self.transform(img_third)\n",
    "\n",
    "        return img_first, img_second, img_third, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "\n",
    "def load_imgs_tenfold_totalframe(video_root, video_list, rectify_label, fold, run_type):\n",
    "    imgs_first = list()\n",
    "    new_imf = list()\n",
    "\n",
    "    ''' Make ten-fold list '''\n",
    "    with open(video_list, 'r') as imf:\n",
    "        imf = imf.readlines()\n",
    "    if run_type == 'train':\n",
    "        fold_ = list(range(1, 11))\n",
    "        fold_.remove(fold)  # [1,2,3,4,5,6,7,8,9, 10] -> [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "        for i in fold_:\n",
    "            fold_str = str(i) + '-fold'  # 1-fold\n",
    "            for index, item in enumerate(\n",
    "                    imf):  # 0, '1-fold\\t31\\n' in {[0, '1-fold\\t31\\n'], [1, 'S037/006 Happy\\n'], ...}\n",
    "                if fold_str in item:  # 1-fold in '1-fold\\t31\\n'\n",
    "                    for j in range(index + 1, index + int(item.split()[1]) + 1):  # (0 + 1, 0 + 31 + 1 )\n",
    "                        new_imf.append(imf[j])  # imf[2] = 'S042/006 Happy\\n'\n",
    "\n",
    "    if run_type == 'test':\n",
    "        fold_ = fold\n",
    "        fold_str = str(fold_) + '-fold'\n",
    "        for index, item in enumerate(imf):\n",
    "            if fold_str in item:\n",
    "                for j in range(index + 1, index + int(item.split()[1]) + 1):\n",
    "                    new_imf.append(imf[j])\n",
    "\n",
    "    index = []\n",
    "    for id, line in enumerate(new_imf):\n",
    "\n",
    "        video_label = line.strip().split()\n",
    "\n",
    "        video_name = video_label[0]  # name of video\n",
    "        try:\n",
    "            label = rectify_label[video_label[1]]  # label of video\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "        video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "        ###  for sampling triple imgs in the single video_path  ####\n",
    "        img_lists = os.listdir(video_path)\n",
    "        img_lists.sort()  # sort files by ascending\n",
    "        \n",
    "        img_lists = img_lists[ - int(round(len(img_lists))) : ]\n",
    "\n",
    "        img_count = len(img_lists)  # number of frames in video\n",
    "        for frame in img_lists:\n",
    "            imgs_first.append((os.path.join(video_path, frame), label))\n",
    "        ###  return video frame index  #####\n",
    "        index.append(np.ones(img_count) * id)\n",
    "\n",
    "    index = np.concatenate(index, axis=0)\n",
    "    return imgs_first, index\n",
    "\n",
    "def load_imgs_tsn_tenfold(video_root, video_list, rectify_label, fold, run_type):\n",
    "    imgs_first = list()\n",
    "    imgs_second = list()\n",
    "    imgs_third = list()\n",
    "    new_imf = list()\n",
    "    ''' Make ten-fold list '''\n",
    "    with open(video_list, 'r') as imf:\n",
    "        imf = imf.readlines()\n",
    "    if run_type == 'train':\n",
    "        fold_ = list(range(1, 11))\n",
    "        fold_.remove(fold)  # [1,2,3,4,5,6,7,8,9,10] -> [2,3,4,5,6,7,8,9,10]\n",
    "        for i in fold_:\n",
    "            fold_str = str(i) + '-fold'  # 1-fold\n",
    "            for index, item in enumerate(\n",
    "                    imf):  # 0, '1-fold\\t31\\n' in {[0, '1-fold\\t31\\n'], [1, 'S037/006 Happy\\n'], ...}\n",
    "                if fold_str in item:  # 1-fold in '1-fold\\t31\\n'\n",
    "                    for j in range(index + 1, index + int(item.split()[1]) + 1):  # (0 + 1, 0 + 31 + 1 )\n",
    "                        new_imf.append(imf[j])  # imf[2] = 'S042/006 Happy\\n'\n",
    "    if run_type == 'test':\n",
    "        fold_ = fold\n",
    "        fold_str = str(fold_) + '-fold'\n",
    "        for index, item in enumerate(imf):\n",
    "            if fold_str in item:\n",
    "                for j in range(index + 1, index + int(item.split()[1]) + 1):\n",
    "                    new_imf.append(imf[j])\n",
    "    ''' Make triple-image list '''\n",
    "    index = []\n",
    "    for id, line in enumerate(new_imf):\n",
    "        video_label = line.strip().split()\n",
    "        video_name = video_label[0]  # name of video\n",
    "        label = rectify_label[video_label[1]]  # label of video\n",
    "        video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "        ###  for sampling triple imgs in the single video_path  ####\n",
    "        img_lists = os.listdir(video_path)\n",
    "        img_lists.sort()  # sort files by ascending\n",
    "        img_lists = img_lists[ - int(round(len(img_lists))):]\n",
    "        img_count = len(img_lists)  # number of frames in video\n",
    "        num_per_part = int(img_count) // 5\n",
    "        if int(img_count) > 5:\n",
    "            for i in range(img_count):\n",
    "                # pdb.set_trace()\n",
    "                random_select_first = random.randint(0, num_per_part)\n",
    "                random_select_second = random.randint(num_per_part, 2 * num_per_part)\n",
    "                random_select_third = random.randint(2 * num_per_part, 3 * num_per_part)\n",
    "\n",
    "                img_path_first = os.path.join(video_path, img_lists[random_select_first])\n",
    "                img_path_second = os.path.join(video_path, img_lists[random_select_second])\n",
    "                img_path_third = os.path.join(video_path, img_lists[random_select_third])\n",
    "\n",
    "                imgs_first.append((img_path_first, label))\n",
    "                imgs_second.append((img_path_second, label))\n",
    "                imgs_third.append((img_path_third, label))\n",
    "\n",
    "        else:\n",
    "            for j in range(len(img_lists)):\n",
    "                img_path_first = os.path.join(video_path, img_lists[j])\n",
    "                img_path_second = os.path.join(video_path, random.choice(img_lists))\n",
    "                img_path_third = os.path.join(video_path, random.choice(img_lists))\n",
    "\n",
    "                imgs_first.append((img_path_first, label))\n",
    "                imgs_second.append((img_path_second, label))\n",
    "                imgs_third.append((img_path_third, label))\n",
    "\n",
    "        ###  return video frame index  #####\n",
    "        index.append(np.ones(img_count) * id)  # id: 0 : 379\n",
    "    index = np.concatenate(index, axis=0)\n",
    "    # index = index.astype(int)\n",
    "    # pdb.set_trace()\n",
    "    return imgs_first, imgs_second, imgs_third, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "137e53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_base(root_train, list_train, batchsize_train, root_eval, list_eval, batchsize_eval, cate2label):\n",
    "    train_dataset = VideoDataset(\n",
    "        video_root=root_train,\n",
    "        video_list=list_train,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]),\n",
    "    )\n",
    "\n",
    "    val_dataset = VideoDataset(\n",
    "        video_root=root_eval,\n",
    "        video_list=list_eval,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.ToTensor()]),\n",
    "        csv=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batchsize_train, shuffle=True,\n",
    "        num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batchsize_eval, shuffle=False,\n",
    "        num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "    return train_loader, val_loader\n",
    "def data_loader_fan(root_train, list_train, batchsize_train, root_eval, list_eval, batchsize_eval, cate2label):\n",
    "\n",
    "    train_dataset = TripleImageDataset(\n",
    "        video_root=root_train,\n",
    "        video_list=list_train,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]),\n",
    "    )\n",
    "\n",
    "    val_dataset = VideoDataset(\n",
    "        video_root=root_eval,\n",
    "        video_list=list_eval,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.ToTensor()]),\n",
    "        csv=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batchsize_train, shuffle=True,\n",
    "        num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batchsize_eval, shuffle=False,\n",
    "        num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d99fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate2label = {'CK+':{0: 'Happy', 1: 'Angry', 2: 'Disgust', 3: 'Fear', 4: 'Sad', 5: 'Contempt', 6: 'Surprise',\n",
    "                     'Angry': 1,'Disgust': 2,'Fear': 3,'Happy': 0,'Contempt': 5,'Sad': 4,'Surprise': 6},\n",
    "\n",
    "              'AFEW':{0: 'Happy',1: 'Angry',2: 'Disgust',3: 'Fear',4: 'Sad',5: 'Neutral',6: 'Surprise',\n",
    "                  'Angry': 1,'Disgust': 2,'Fear': 3,'Happy': 0,'Neutral': 5,'Sad': 4,'Surprise': 6},\n",
    "\n",
    "              'RAVDESS':{0: 'neutral',1: 'calm',2: 'happy',3: 'sad',4: 'angry',5: 'fearful',6: 'disgust',7: 'surprised',\n",
    "                  'neutral': 0,'calm': 1,'happy': 2,'sad': 3,'angry': 4,'fearful': 5,'disgust': 6,'surprised': 7}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11476058",
   "metadata": {},
   "source": [
    "### Load data !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbfcd7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x10eda50c910>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root directory of train data\n",
    "root_train = './data/face/train_ravdess'\n",
    "# txt file train list\n",
    "list_train = './data/txt/train_ravdess.txt'\n",
    "batchsize_train= 48\n",
    "root_eval = './data/face/test_ravdess'\n",
    "list_eval = './data/txt/test_ravdess.txt'\n",
    "batchsize_eval= 64\n",
    "train_loader, val_loader = data_loader_fan(root_train, list_train, batchsize_train, root_eval,\n",
    "                                               list_eval, batchsize_eval, cate2label['RAVDESS'])\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdb2929b-97dd-4da3-9de3-ebd1c88d783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x10eda50ca60>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf65ed",
   "metadata": {},
   "source": [
    "## Load pretrain model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aec8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9056f-2c95-4117-b09e-9d03590ba90e",
   "metadata": {},
   "source": [
    "### model construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "316be5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def norm_angle(angle):\n",
    "    norm_angle = sigmoid(10 * (abs(angle) / 0.7853975 - 1))\n",
    "    return norm_angle\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "###''' self-attention; relation-attention '''\n",
    "\n",
    "class ResNet_AT(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, end2end=True, at_type=''):\n",
    "        self.inplanes = 64\n",
    "        self.end2end = end2end\n",
    "        super(ResNet_AT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.alpha = nn.Sequential(nn.Linear(512, 1),\n",
    "                                   nn.Sigmoid())\n",
    "\n",
    "        self.beta = nn.Sequential(nn.Linear(1024, 1),\n",
    "                                  nn.Sigmoid())\n",
    "\n",
    "        self.pred_fc1 = nn.Linear(512, 7)\n",
    "        self.pred_fc2 = nn.Linear(1024, 7)\n",
    "        self.at_type = at_type\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x='', phrase='train', AT_level='first_level',vectors='',vm='',alphas_from1='',index_matrix=''):\n",
    "\n",
    "        vs = []\n",
    "        alphas = []\n",
    "\n",
    "        assert phrase == 'train' or phrase == 'eval'\n",
    "        assert AT_level == 'first_level' or AT_level == 'second_level' or AT_level == 'pred'\n",
    "        if phrase == 'train':\n",
    "            num_pair = 3\n",
    "\n",
    "            for i in range(num_pair):\n",
    "                f = x[:, :, :, :, i]  # x[128,3,224,224]\n",
    "\n",
    "                f = self.conv1(f)\n",
    "                f = self.bn1(f)\n",
    "                f = self.relu(f)\n",
    "                f = self.maxpool(f)\n",
    "\n",
    "                f = self.layer1(f)\n",
    "                f = self.layer2(f)\n",
    "                f = self.layer3(f)\n",
    "                f = self.layer4(f)\n",
    "                f = self.avgpool(f)\n",
    "\n",
    "                f = f.squeeze(3).squeeze(2)  # f[1, 512, 1, 1] ---> f[1, 512]\n",
    "\n",
    "                # MN_MODEL(first Level)\n",
    "                vs.append(f)\n",
    "                alphas.append(self.alpha(self.dropout(f)))\n",
    "\n",
    "            vs_stack = torch.stack(vs, dim=2)\n",
    "            alphas_stack = torch.stack(alphas, dim=2)\n",
    "\n",
    "            if self.at_type == 'self-attention':\n",
    "                vm1 = vs_stack.mul(alphas_stack).sum(2).div(alphas_stack.sum(2))\n",
    "            if self.at_type == 'self_relation-attention':\n",
    "                vm1 = vs_stack.mul(alphas_stack).sum(2).div(alphas_stack.sum(2))\n",
    "                betas = []\n",
    "                for i in range(len(vs)):\n",
    "                    vs[i] = torch.cat([vs[i], vm1], dim=1)\n",
    "                    betas.append(self.beta(self.dropout(vs[i])))\n",
    "\n",
    "                cascadeVs_stack = torch.stack(vs, dim=2)\n",
    "                betas_stack = torch.stack(betas, dim=2)\n",
    "                output = cascadeVs_stack.mul(betas_stack * alphas_stack).sum(2).div((betas_stack * alphas_stack).sum(2))\n",
    "\n",
    "            if self.at_type == 'self-attention':\n",
    "                vm1 = self.dropout(vm1)\n",
    "                pred_score = self.pred_fc1(vm1)\n",
    "\n",
    "            if self.at_type == 'self_relation-attention':\n",
    "                output = self.dropout2(output)\n",
    "                pred_score = self.pred_fc2(output)\n",
    "\n",
    "            return pred_score\n",
    "\n",
    "        if phrase == 'eval':\n",
    "            if AT_level == 'first_level':\n",
    "                f = self.conv1(x)\n",
    "                f = self.bn1(f)\n",
    "                f = self.relu(f)\n",
    "                f = self.maxpool(f)\n",
    "\n",
    "                f = self.layer1(f)\n",
    "                f = self.layer2(f)\n",
    "                f = self.layer3(f)\n",
    "                f = self.layer4(f)\n",
    "                f = self.avgpool(f)\n",
    "\n",
    "                f = f.squeeze(3).squeeze(2)  # f[1, 512, 1, 1] ---> f[1, 512]\n",
    "                # MN_MODEL(first Level)\n",
    "                alphas = self.alpha(self.dropout(f))\n",
    "\n",
    "                return f, alphas\n",
    "\n",
    "            if AT_level == 'second_level':\n",
    "                assert self.at_type == 'self_relation-attention'\n",
    "                vms = index_matrix.permute(1, 0).mm(vm)  # [381, 21783] -> [21783,381] * [381,512] --> [21783, 512]\n",
    "                vs_cate = torch.cat([vectors, vms], dim=1)\n",
    "\n",
    "                betas = self.beta(self.dropout(vs_cate))\n",
    "                ''' keywords: mean_fc ; weight_sourcefc; sum_alpha; weightmean_sourcefc '''\n",
    "                ''' alpha * beta '''\n",
    "                weight_catefc = vs_cate.mul(alphas_from1)  # [21570,512] * [21570,1] --->[21570,512]\n",
    "                alpha_beta = alphas_from1.mul(betas)\n",
    "                sum_alphabetas = index_matrix.mm(alpha_beta)  # [380,21570] * [21570,1] -> [380,1]\n",
    "                weightmean_catefc = index_matrix.mm(weight_catefc).div(sum_alphabetas)\n",
    "\n",
    "                weightmean_catefc = self.dropout2(weightmean_catefc)\n",
    "                pred_score = self.pred_fc2(weightmean_catefc)\n",
    "\n",
    "                return pred_score\n",
    "\n",
    "            if AT_level == 'pred':\n",
    "                if self.at_type == 'self-attention':\n",
    "                    pred_score = self.pred_fc1(self.dropout(vm))\n",
    "\n",
    "                return pred_score\n",
    "\n",
    "''' self-attention; relation-attention '''\n",
    "def resnet18_at(**kwargs):\n",
    "    # Constructs base a ResNet-18 model.\n",
    "    model = ResNet_AT(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5e150-ba41-4c21-9b23-3dfe8b2b52af",
   "metadata": {},
   "source": [
    "### pretrain model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "985f33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_parameters(_structure, _parameterDir):\n",
    "\n",
    "    checkpoint = torch.load(_parameterDir)\n",
    "    pretrained_state_dict = checkpoint['state_dict']\n",
    "    model_state_dict = _structure.state_dict()\n",
    "\n",
    "    for key in pretrained_state_dict:\n",
    "        if ((key == 'module.fc.weight') | (key == 'module.fc.bias')):\n",
    "\n",
    "            pass\n",
    "        else:\n",
    "            model_state_dict[key.replace('module.', '')] = pretrained_state_dict[key]\n",
    "\n",
    "    _structure.load_state_dict(model_state_dict)\n",
    "    model = torch.nn.DataParallel(_structure).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faccd7-99d7-4c36-b9d2-25b1b57ce921",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdcd5373-3d18-4791-a67c-1fcda019f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34085130-abd5-4527-b7f6-358f6ac85cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)  # first position is score; second position is pred.\n",
    "    pred = pred.t()  # .t() is T of matrix (256 * 1) -> (1 * 256)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))  # target.view(1,2,2,-1): (256,) -> (1, 2, 2, 64)\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, at_type=''):\n",
    "\n",
    "    if not os.path.exists('./model'):\n",
    "        os.makedirs('./model')\n",
    "\n",
    "    epoch = state['epoch']\n",
    "    save_dir = './model/'+at_type+'_' + str(epoch) + '_' + str(round(float(state['accuracy']), 4))\n",
    "    torch.save(state, save_dir)\n",
    "    print(save_dir)\n",
    "    \n",
    "def time_now():\n",
    "  ISOTIMEFORMAT='%d-%h-%Y-%H-%M-%S'\n",
    "  string = '{:}'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n",
    "  return string\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir, title):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.log_dir = Path(\"{:}\".format(str(log_dir)))\n",
    "        if not self.log_dir.exists(): os.makedirs(str(self.log_dir))\n",
    "        self.title = title\n",
    "        self.log_file = '{:}/{:}_date_{:}.txt'.format(self.log_dir,title, time_now())\n",
    "        self.file_writer = open(self.log_file, 'a')\n",
    "        \n",
    "        \n",
    "    def print(self, string, fprint=True, is_pp=False):\n",
    "        if is_pp: pp.pprint (string)\n",
    "        else:     print(string)\n",
    "        if fprint:\n",
    "          self.file_writer.write('{:}\\n'.format(string))\n",
    "          self.file_writer.flush()\n",
    "            \n",
    "    def write(self, string):\n",
    "        self.file_writer.write('{:}\\n'.format(string))\n",
    "        self.file_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c343a85-0369-4031-887f-9beed6479cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Start the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda90ef-8206-446a-8861-bed93c23d4bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2394b58f-0f55-43a7-aa51-e2bd4e3fd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d6ff406-59a0-4a78-ba08-4487373b0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./log/','jupyter_rav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a5e1b0d-c393-4ab4-99ac-21dcb80ef3d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "761428e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_type = ['self-attention', 'self_relation-attention']\n",
    "# select attention type\n",
    "at_type = at_type[1]\n",
    "lr = 1e-4\n",
    "epochs = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6d1ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_structure = resnet18_at(at_type=at_type)\n",
    "_parameterDir = './pretrain_model/Resnet18_FER+_pytorch.pth.tar'\n",
    "model = model_parameters(_structure, _parameterDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e704ef27-4c69-452e-9291-e32341fcf2c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet_AT(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.6, inplace=False)\n",
       "    (alpha): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (beta): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (pred_fc1): Linear(in_features=512, out_features=7, bias=True)\n",
       "    (pred_fc2): Linear(in_features=1024, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca5779",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1708a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.2)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a31ec-2ecf-4385-b203-95ff6860fefd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "869dd67b-c01a-486a-b040-ed12031e6bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch, logger):\n",
    "    losses = AverageMeter()\n",
    "    topframe = AverageMeter()\n",
    "    topVideo = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    output_store_fc = []\n",
    "    target_store = []\n",
    "    index_vector = []\n",
    "    \n",
    "    model.train()\n",
    "    for i, (input_first, input_second, input_third, target_first, index) in enumerate(train_loader):\n",
    "        target_var = target_first.to(DEVICE)\n",
    "        input_var = torch.stack([input_first, input_second , input_third], dim=4).to(DEVICE)\n",
    "        # compute output\n",
    "        ''' model & full_model'''\n",
    "        pred_score = model(input_var)\n",
    "        loss = F.cross_entropy(pred_score, target_var)\n",
    "        loss = loss.sum()\n",
    "        #\n",
    "        output_store_fc.append(pred_score)\n",
    "        target_store.append(target_var)\n",
    "        index_vector.append(index)\n",
    "        # measure accuracy and record loss\n",
    "        acc_iter = accuracy(pred_score.data, target_var, topk=(1,))\n",
    "        losses.update(loss.item(), input_var.size(0))\n",
    "        topframe.update(acc_iter[0], input_var.size(0))\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            logger.print('Epoch: [{:3d}][{:3d}/{:3d}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@1 {topframe.val:.3f} ({topframe.avg:.3f})\\t'\n",
    "                .format(\n",
    "                epoch, i, len(train_loader), loss=losses, topframe=topframe))\n",
    "\n",
    "    index_vector = torch.cat(index_vector, dim=0)  # [256] ... [256]  --->  [21570]\n",
    "    index_matrix = []\n",
    "    for i in range(int(max(index_vector)) + 1):\n",
    "        index_matrix.append(index_vector == i)\n",
    "\n",
    "    index_matrix = torch.stack(index_matrix, dim=0).to(DEVICE).float()  # [21570]  --->  [380, 21570]\n",
    "    output_store_fc = torch.cat(output_store_fc, dim=0)  # [256,7] ... [256,7]  --->  [21570, 7]\n",
    "    target_store = torch.cat(target_store, dim=0).float()  # [256] ... [256]  --->  [21570]\n",
    "    pred_matrix_fc = index_matrix.mm(output_store_fc)  # [380,21570] * [21570, 7] = [380,7]\n",
    "    target_vector = index_matrix.mm(target_store.unsqueeze(1)).squeeze(1).div(\n",
    "        index_matrix.sum(1)).long()  # [380,21570] * [21570,1] -> [380,1] / sum([21570,1]) -> [380]\n",
    "\n",
    "    acc_video = accuracy(pred_matrix_fc.cpu(), target_vector.cpu(), topk=(1,))\n",
    "    topVideo.update(acc_video[0], i + 1)\n",
    "    logger.print(' *Acc@Video {topVideo.avg:.3f}   *Acc@Frame {topframe.avg:.3f} '.format(topVideo=topVideo, topframe=topframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a844ce5-a6fe-4995-9d05-6c5c521d2a62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a7eb416-bf4e-4206-afc4-8af67edd385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_loader, model, at_type, logger):\n",
    "    topVideo = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    output_store_fc = []\n",
    "    output_alpha    = []\n",
    "    target_store = []\n",
    "    index_vector = []\n",
    "    with torch.no_grad():\n",
    "        num_classes = 8\n",
    "        class_metrics = {i: {'tp': 0, 'total': 0} for i in range(num_classes)}\n",
    "        class_metrics2 = {i: {'tp': 0, 'fp': 0, 'fn': 0} for i in range(num_classes)}\n",
    "        for i, (input_var, target, index) in enumerate(val_loader):\n",
    "            # compute output\n",
    "            target = target.to(DEVICE)\n",
    "            input_var = input_var.to(DEVICE)\n",
    "            ''' model & full_model'''\n",
    "            f, alphas = model(input_var, phrase = 'eval')\n",
    "\n",
    "            output_store_fc.append(f)\n",
    "            output_alpha.append(alphas)\n",
    "            target_store.append(target)\n",
    "            index_vector.append(index)\n",
    "\n",
    "        index_vector = torch.cat(index_vector, dim=0)  # [256] ... [256]  --->  [21570]\n",
    "        index_matrix = []\n",
    "        for i in range(int(max(index_vector)) + 1):\n",
    "            index_matrix.append(index_vector == i)\n",
    "\n",
    "        index_matrix = torch.stack(index_matrix, dim=0).to(DEVICE).float()  # [21570]  --->  [380, 21570]\n",
    "        output_store_fc = torch.cat(output_store_fc, dim=0)  # [256,7] ... [256,7]  --->  [21570, 7]\n",
    "        output_alpha    = torch.cat(output_alpha, dim=0)     # [256,1] ... [256,1]  --->  [21570, 1]\n",
    "        target_store = torch.cat(target_store, dim=0).float()  # [256] ... [256]  --->  [21570]\n",
    "        ''' keywords: mean_fc ; weight_sourcefc; sum_alpha; weightmean_sourcefc '''\n",
    "        weight_sourcefc = output_store_fc.mul(output_alpha)   #[21570,512] * [21570,1] --->[21570,512]\n",
    "        sum_alpha = index_matrix.mm(output_alpha) # [380,21570] * [21570,1] -> [380,1]\n",
    "        weightmean_sourcefc = index_matrix.mm(weight_sourcefc).div(sum_alpha)\n",
    "        target_vector = index_matrix.mm(target_store.unsqueeze(1)).squeeze(1).div(\n",
    "            index_matrix.sum(1)).long()  # [380,21570] * [21570,1] -> [380,1] / sum([21570,1]) -> [380]\n",
    "        if at_type == 'self-attention':\n",
    "            pred_score = model(vm=weightmean_sourcefc, phrase='eval', AT_level='pred')\n",
    "        if at_type == 'self_relation-attention':\n",
    "            pred_score  = model(vectors=output_store_fc, vm=weightmean_sourcefc, alphas_from1=output_alpha, index_matrix=index_matrix, phrase='eval', AT_level='second_level')\n",
    "        pred = pred_score.argmax(dim=1)\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            class_metrics[i]['tp'] += ((pred == i) & (target_vector == i)).sum().item()\n",
    "            class_metrics[i]['total'] += (target_vector == i).sum().item()\n",
    "\n",
    "        # Compute the accuracy for each class\n",
    "        for i in range(num_classes):\n",
    "            tp = class_metrics[i]['tp']\n",
    "            total = class_metrics[i]['total']\n",
    "            accuracy = tp / total if total > 0 else 0\n",
    "            print(f'Class {i}: Accuracy: {accuracy:.3f}')\n",
    "        # Update the class metrics\n",
    "        pred = pred_score.argmax(dim=1)\n",
    "        for i in range(num_classes):\n",
    "            class_metrics2[i]['tp'] += ((pred == i) & (target_vector == i)).sum().item()\n",
    "            class_metrics2[i]['fp'] += ((pred == i) & (target_vector != i)).sum().item()\n",
    "            class_metrics2[i]['fn'] += ((pred != i) & (target_vector == i)).sum().item()\n",
    "\n",
    "        # Compute the precision, recall, and F1 score for each class\n",
    "        for i in range(num_classes):\n",
    "            tp = class_metrics2[i]['tp']\n",
    "            fp = class_metrics2[i]['fp']\n",
    "            fn = class_metrics2[i]['fn']\n",
    "            precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "            print(f'Class {i}: Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}')\n",
    "            \n",
    "            \n",
    "        acc_video = accuracy(pred_score.cpu(), target_vector.cpu(), topk=(1,))\n",
    "        topVideo.update(acc_video[0], i + 1)\n",
    "        logger.print(' *Acc@Video {topVideo.avg:.3f} '.format(topVideo=topVideo))\n",
    "        return topVideo.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c16bc-e79a-4e3b-8140-d65bc781285f",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2f1cb67-2295-4bc1-99b2-2c70af1add37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame attention network (fan) afew dataset, learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "logger.print('frame attention network (fan) afew dataset, learning rate: {:}'.format(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06654e7-a4cc-402d-b57e-5b4ab4f56d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0\n",
    "# train(train_loader, model, optimizer, 1, logger)\n",
    "# acc_epoch = val(val_loader, model, at_type, logger)\n",
    "# is_best = acc_epoch > best_acc\n",
    "# if is_best:\n",
    "#     logger.print('better model!')\n",
    "#     best_acc = max(acc_epoch, best_acc)\n",
    "#     save_checkpoint({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'state_dict': model.state_dict(),\n",
    "#             'accuracy': acc_epoch,\n",
    "#         }, at_type=at_type)\n",
    "        \n",
    "# lr_scheduler.step()\n",
    "# logger.print(\"epoch: {:} learning rate:{:}\".format(epoch+1, optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff336425-974e-4371-bec1-2881652a6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, optimizer, epoch, logger)\n",
    "    acc_epoch = val(val_loader, model, at_type, logger)\n",
    "    is_best = acc_epoch > best_acc\n",
    "    if is_best:\n",
    "        logger.print('better model!')\n",
    "        best_acc = max(acc_epoch, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'accuracy': acc_epoch,\n",
    "        }, at_type=at_type)\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    logger.print(\"epoch: {:} learning rate:{:}\".format(epoch+1, optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618a5a7-9ed2-4e07-bedb-e49945fdbe17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
