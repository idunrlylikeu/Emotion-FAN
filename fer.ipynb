{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf011fc-e30e-4c9b-ba1f-21692ca3355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44d5be-3ed6-4354-aa23-9feea522bf40",
   "metadata": {},
   "source": [
    "# Data split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852681d-107b-4f99-9424-e713ea280d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## path + calculates how many train data per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99158a-329c-405b-ba95-1da0c03cb0a7",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1510e2-6e07-464d-a71d-95639a59664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data per class [32. 32.  0. 32. 32. 16. 32.  0.]\n",
      "data per test [10. 10.  0. 10. 10.  5. 10.  0.]\n"
     ]
    }
   ],
   "source": [
    "def get_files_from_folder(path):\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    return np.asarray(files)\n",
    "\n",
    "# Path to data\n",
    "path_to_data = \"./data/video/train_ravdess\"\n",
    "# Path to test data where to save\n",
    "path_to_test_data = \"./data/video/test_ravdess\"\n",
    "# Train ratio - 0.7 means splitting data in 70 % train and 30 % test\n",
    "train_ratio = float(0.7)\n",
    "\n",
    "# get dirs\n",
    "_, dirs, _ = next(os.walk(path_to_data))\n",
    "\n",
    "# calculates how many train data per class\n",
    "data_counter_per_class = np.zeros((len(dirs)))\n",
    "for i in range(len(dirs)):\n",
    "    path = os.path.join(path_to_data, dirs[i])\n",
    "    files = get_files_from_folder(path)\n",
    "    data_counter_per_class[i] = len(files)\n",
    "print(\"data per class\", data_counter_per_class)\n",
    "test_counter = np.round(data_counter_per_class * (1 - train_ratio))\n",
    "print(\"data per test\", test_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d13cc-46bd-4478-87dc-6b033ee56e94",
   "metadata": {},
   "source": [
    "## transfers files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5f8b21-a722-4972-829d-bfcd68d687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfers files\n",
    "for i in range(len(dirs)):\n",
    "    path_to_original = os.path.join(path_to_data, dirs[i])\n",
    "    path_to_save = os.path.join(path_to_test_data, dirs[i])\n",
    "\n",
    "    #creates dir\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    files = get_files_from_folder(path_to_original)\n",
    "                                 \n",
    "    # random file sequence\n",
    "    np.random.shuffle(files)\n",
    "                                  \n",
    "    # moves data\n",
    "    for j in range(int(test_counter[i])):\n",
    "        dst = os.path.join(path_to_save, files[j])\n",
    "        src = os.path.join(path_to_original, files[j])\n",
    "        shutil.move(src, dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3567e",
   "metadata": {},
   "source": [
    "# Face alignment part\n",
    "## Preprocess video to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e17154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import threading\n",
    "import pdb\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16b8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_threads(threads, n_thread):\n",
    "    used_thread = []\n",
    "    for num, new_thread in enumerate(threads):\n",
    "        print('thread index: {:}'.format(num), end=' \\t')\n",
    "        new_thread.start()\n",
    "        used_thread.append(new_thread)\n",
    "        \n",
    "        if num % n_thread == 0:\n",
    "            for old_thread in used_thread:\n",
    "                old_thread.join()\n",
    "            used_thread = []\n",
    "\n",
    "class threadFun(threading.Thread):\n",
    "    def __init__(self, func, args):\n",
    "        super(threadFun, self).__init__()\n",
    "        self.fun = func\n",
    "        self.args = args\n",
    "    def run(self):\n",
    "        self.fun(*self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_EXTENSIONS = ['mp4', 'webm', 'avi']\n",
    "\n",
    "# check file name, is it end with video type extension\n",
    "def is_video_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in VIDEO_EXTENSIONS)\n",
    "\n",
    "# helper function to make a directory if not exist\n",
    "def makefile(file_dir):\n",
    "    if not os.path.exists(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "\n",
    "# use ffmpeg to extract video to frame to the frame_output\n",
    "def video2frame(video_input, frame_output):\n",
    "    linux_commod = 'ffmpeg -i {:} -f image2 {:}/%07d.jpg'.format(video_input, frame_output)\n",
    "    print('{:}'.format(video_input))\n",
    "    subprocess.getstatusoutput(linux_commod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4784f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vdo2frame(video_dir, frame_dir, n_thread):\n",
    "    print('Starting: convert videos into frames\\nvideo_dir: {:}\\tframe_dir: {:}'.format(video_dir, frame_dir))\n",
    "    threads = []\n",
    "    for root, dirs, files in os.walk(video_dir):\n",
    "        for file_name in files:\n",
    "            if is_video_file(file_name):\n",
    "                # get video name and path\n",
    "                video_name = os.path.join(root, file_name)\n",
    "                # create frame output path from changing video directory in video_name to frame_dir and split the file extension\n",
    "                frame_output_path = os.path.splitext(video_name.replace(video_dir, frame_dir))[0]\n",
    "                makefile(frame_output_path)\n",
    "                threads.append(threadFun(video2frame, (video_name, frame_output_path)))\n",
    "    run_threads(threads, n_thread)\n",
    "    print('all threads is finished') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fb4fa-ae6e-474d-b9cd-6b48b9cf1cb6",
   "metadata": {},
   "source": [
    "### !set path\n",
    "Do on both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adc1a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert videos into frames\n",
      "video_dir: ./data/video/train_ravdess\tframe_dir: ./data/frame/train_ravdess\n",
      "thread index: 0 \t./data/video/train_ravdess\\angry\\01-02-05-01-01-01-04.mp4\n",
      "thread index: 1 \t./data/video/train_ravdess\\angry\\01-02-05-01-01-02-04.mp4\n",
      "thread index: 2 \t./data/video/train_ravdess\\angry\\01-02-05-01-02-01-04.mp4\n",
      "thread index: 3 \t./data/video/train_ravdess\\angry\\01-02-05-01-02-02-04.mp4\n",
      "thread index: 4 \t./data/video/train_ravdess\\angry\\01-02-05-02-02-01-04.mp4\n",
      "thread index: 5 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-01-01.mp4\n",
      "thread index: 6 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-01-02.mp4\n",
      "thread index: 7 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-02-01.mp4\n",
      "thread index: 8 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-02-02.mp4\n",
      "thread index: 9 \t./data/video/train_ravdess\\angry\\02-02-05-01-01-02-03.mp4\n",
      "thread index: 10 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-01-02.mp4\n",
      "thread index: 11 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-01-03.mp4\n",
      "thread index: 12 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-02-01.mp4\n",
      "thread index: 13 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-02-02.mp4\n",
      "thread index: 14 \t./data/video/train_ravdess\\angry\\02-02-05-01-02-02-03.mp4\n",
      "thread index: 15 \t./data/video/train_ravdess\\angry\\02-02-05-02-01-01-02.mp4\n",
      "thread index: 16 \t./data/video/train_ravdess\\angry\\02-02-05-02-01-01-03.mp4\n",
      "thread index: 17 \t./data/video/train_ravdess\\angry\\02-02-05-02-01-02-01.mp4\n",
      "thread index: 18 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-01-03.mp4\n",
      "thread index: 19 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-02-01.mp4\n",
      "thread index: 20 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-02-02.mp4\n",
      "thread index: 21 \t./data/video/train_ravdess\\angry\\02-02-05-02-02-02-03.mp4\n",
      "thread index: 22 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-01-01.mp4\n",
      "thread index: 23 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-01-02.mp4\n",
      "thread index: 24 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-02-01.mp4\n",
      "thread index: 25 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-02-02.mp4\n",
      "thread index: 26 \t./data/video/train_ravdess\\calm\\02-02-02-01-01-02-04.mp4\n",
      "thread index: 27 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-01-01.mp4\n",
      "thread index: 28 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-01-02.mp4\n",
      "thread index: 29 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-01-03.mp4\n",
      "thread index: 30 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-02-01.mp4\n",
      "thread index: 31 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-02-02.mp4\n",
      "thread index: 32 \t./data/video/train_ravdess\\calm\\02-02-02-01-02-02-03.mp4\n",
      "thread index: 33 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-01-02.mp4\n",
      "thread index: 34 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-01-03.mp4\n",
      "thread index: 35 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-01-04.mp4\n",
      "thread index: 36 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-02-01.mp4\n",
      "thread index: 37 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-02-02.mp4\n",
      "thread index: 38 \t./data/video/train_ravdess\\calm\\02-02-02-02-01-02-04.mp4\n",
      "thread index: 39 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-01-02.mp4\n",
      "thread index: 40 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-01-03.mp4\n",
      "thread index: 41 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-01-04.mp4\n",
      "thread index: 42 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-02-01.mp4\n",
      "thread index: 43 \t./data/video/train_ravdess\\calm\\02-02-02-02-02-02-04.mp4\n",
      "thread index: 44 \t./data/video/train_ravdess\\fearful\\01-02-06-01-01-01-04.mp4\n",
      "thread index: 45 \t./data/video/train_ravdess\\fearful\\01-02-06-01-01-02-04.mp4\n",
      "thread index: 46 \t./data/video/train_ravdess\\fearful\\01-02-06-01-02-02-04.mp4\n",
      "thread index: 47 \t./data/video/train_ravdess\\fearful\\01-02-06-02-02-01-04.mp4\n",
      "thread index: 48 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-01-02.mp4\n",
      "thread index: 49 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-01-03.mp4\n",
      "thread index: 50 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-02-01.mp4\n",
      "thread index: 51 \t./data/video/train_ravdess\\fearful\\02-02-06-01-01-02-03.mp4\n",
      "thread index: 52 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-01-01.mp4\n",
      "thread index: 53 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-01-02.mp4\n",
      "thread index: 54 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-01-03.mp4\n",
      "thread index: 55 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-02-01.mp4\n",
      "thread index: 56 \t./data/video/train_ravdess\\fearful\\02-02-06-01-02-02-03.mp4\n",
      "thread index: 57 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-01-01.mp4\n",
      "thread index: 58 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-02-01.mp4\n",
      "thread index: 59 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-02-02.mp4\n",
      "thread index: 60 \t./data/video/train_ravdess\\fearful\\02-02-06-02-01-02-03.mp4\n",
      "thread index: 61 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-01-02.mp4\n",
      "thread index: 62 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-01-03.mp4\n",
      "thread index: 63 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-02-01.mp4\n",
      "thread index: 64 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-02-02.mp4\n",
      "thread index: 65 \t./data/video/train_ravdess\\fearful\\02-02-06-02-02-02-03.mp4\n",
      "thread index: 66 \t./data/video/train_ravdess\\happy\\01-02-03-01-01-01-04.mp4\n",
      "thread index: 67 \t./data/video/train_ravdess\\happy\\01-02-03-01-01-02-04.mp4\n",
      "thread index: 68 \t./data/video/train_ravdess\\happy\\01-02-03-02-01-01-04.mp4\n",
      "thread index: 69 \t./data/video/train_ravdess\\happy\\01-02-03-02-01-02-04.mp4\n",
      "thread index: 70 \t./data/video/train_ravdess\\happy\\01-02-03-02-02-01-04.mp4\n",
      "thread index: 71 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-01-02.mp4\n",
      "thread index: 72 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-01-03.mp4\n",
      "thread index: 73 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-02-01.mp4\n",
      "thread index: 74 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-02-02.mp4\n",
      "thread index: 75 \t./data/video/train_ravdess\\happy\\02-02-03-01-01-02-03.mp4\n",
      "thread index: 76 \t./data/video/train_ravdess\\happy\\02-02-03-01-02-01-01.mp4\n",
      "thread index: 77 \t./data/video/train_ravdess\\happy\\02-02-03-01-02-01-02.mp4\n",
      "thread index: 78 \t./data/video/train_ravdess\\happy\\02-02-03-01-02-02-03.mp4\n",
      "thread index: 79 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-01-01.mp4\n",
      "thread index: 80 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-01-02.mp4\n",
      "thread index: 81 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-01-03.mp4\n",
      "thread index: 82 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-02-02.mp4\n",
      "thread index: 83 \t./data/video/train_ravdess\\happy\\02-02-03-02-01-02-03.mp4\n",
      "thread index: 84 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-01-03.mp4\n",
      "thread index: 85 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-02-01.mp4\n",
      "thread index: 86 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-02-02.mp4\n",
      "thread index: 87 \t./data/video/train_ravdess\\happy\\02-02-03-02-02-02-03.mp4\n",
      "thread index: 88 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-01-01.mp4\n",
      "thread index: 89 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-01-03.mp4\n",
      "thread index: 90 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-02-01.mp4\n",
      "thread index: 91 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-02-02.mp4\n",
      "thread index: 92 \t./data/video/train_ravdess\\neutral\\02-02-01-01-01-02-04.mp4\n",
      "thread index: 93 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-01-02.mp4\n",
      "thread index: 94 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-01-03.mp4\n",
      "thread index: 95 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-01-04.mp4\n",
      "thread index: 96 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-02-02.mp4\n",
      "thread index: 97 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-02-03.mp4\n",
      "thread index: 98 \t./data/video/train_ravdess\\neutral\\02-02-01-01-02-02-04.mp4\n",
      "thread index: 99 \t./data/video/train_ravdess\\sad\\01-02-04-01-01-01-04.mp4\n",
      "thread index: 100 \t./data/video/train_ravdess\\sad\\01-02-04-01-01-02-04.mp4\n",
      "thread index: 101 \t./data/video/train_ravdess\\sad\\01-02-04-01-02-02-04.mp4\n",
      "thread index: 102 \t./data/video/train_ravdess\\sad\\01-02-04-02-01-01-04.mp4\n",
      "thread index: 103 \t./data/video/train_ravdess\\sad\\01-02-04-02-01-02-04.mp4\n",
      "thread index: 104 \t./data/video/train_ravdess\\sad\\01-02-04-02-02-02-04.mp4\n",
      "thread index: 105 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-01-01.mp4\n",
      "thread index: 106 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-01-02.mp4\n",
      "thread index: 107 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-01-03.mp4\n",
      "thread index: 108 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-02-01.mp4\n",
      "thread index: 109 \t./data/video/train_ravdess\\sad\\02-02-04-01-01-02-02.mp4\n",
      "thread index: 110 \t./data/video/train_ravdess\\sad\\02-02-04-01-02-01-02.mp4\n",
      "thread index: 111 \t./data/video/train_ravdess\\sad\\02-02-04-01-02-01-03.mp4\n",
      "thread index: 112 \t./data/video/train_ravdess\\sad\\02-02-04-01-02-02-01.mp4\n",
      "thread index: 113 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-01-01.mp4\n",
      "thread index: 114 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-01-03.mp4\n",
      "thread index: 115 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-02-02.mp4\n",
      "thread index: 116 \t./data/video/train_ravdess\\sad\\02-02-04-02-01-02-03.mp4\n",
      "thread index: 117 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-01-01.mp4\n",
      "thread index: 118 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-01-03.mp4\n",
      "thread index: 119 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-02-01.mp4\n",
      "thread index: 120 \t./data/video/train_ravdess\\sad\\02-02-04-02-02-02-02.mp4\n",
      "all threads is finished\n"
     ]
    }
   ],
   "source": [
    "video_dir_train ='./data/video/train_ravdess'\n",
    "frame_dir_train = './data/frame/train_ravdess'\n",
    "run_vdo2frame(video_dir_train, frame_dir_train, n_thread =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec702d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: convert videos into frames\n",
      "video_dir: ./data/video/test_ravdess\tframe_dir: ./data/frame/test_ravdess\n",
      "thread index: 0 \t./data/video/test_ravdess\\angry\\01-02-05-02-01-01-04.mp4\n",
      "thread index: 1 \t./data/video/test_ravdess\\angry\\01-02-05-02-01-02-04.mp4\n",
      "thread index: 2 \t./data/video/test_ravdess\\angry\\01-02-05-02-02-02-04.mp4\n",
      "thread index: 3 \t./data/video/test_ravdess\\angry\\02-02-05-01-01-01-03.mp4\n",
      "thread index: 4 \t./data/video/test_ravdess\\angry\\02-02-05-01-02-01-01.mp4\n",
      "thread index: 5 \t./data/video/test_ravdess\\angry\\02-02-05-02-01-01-01.mp4\n",
      "thread index: 6 \t./data/video/test_ravdess\\angry\\02-02-05-02-01-02-02.mp4\n",
      "thread index: 7 \t./data/video/test_ravdess\\angry\\02-02-05-02-01-02-03.mp4\n",
      "thread index: 8 \t./data/video/test_ravdess\\angry\\02-02-05-02-02-01-01.mp4\n",
      "thread index: 9 \t./data/video/test_ravdess\\angry\\02-02-05-02-02-01-02.mp4\n",
      "thread index: 10 \t./data/video/test_ravdess\\calm\\02-02-02-01-01-01-03.mp4\n",
      "thread index: 11 \t./data/video/test_ravdess\\calm\\02-02-02-01-01-01-04.mp4\n",
      "thread index: 12 \t./data/video/test_ravdess\\calm\\02-02-02-01-01-02-03.mp4\n",
      "thread index: 13 \t./data/video/test_ravdess\\calm\\02-02-02-01-02-01-04.mp4\n",
      "thread index: 14 \t./data/video/test_ravdess\\calm\\02-02-02-01-02-02-04.mp4\n",
      "thread index: 15 \t./data/video/test_ravdess\\calm\\02-02-02-02-01-01-01.mp4\n",
      "thread index: 16 \t./data/video/test_ravdess\\calm\\02-02-02-02-01-02-03.mp4\n",
      "thread index: 17 \t./data/video/test_ravdess\\calm\\02-02-02-02-02-01-01.mp4\n",
      "thread index: 18 \t./data/video/test_ravdess\\calm\\02-02-02-02-02-02-02.mp4\n",
      "thread index: 19 \t./data/video/test_ravdess\\calm\\02-02-02-02-02-02-03.mp4\n",
      "thread index: 20 \t./data/video/test_ravdess\\fearful\\01-02-06-01-02-01-04.mp4\n",
      "thread index: 21 \t./data/video/test_ravdess\\fearful\\01-02-06-02-01-01-04.mp4\n",
      "thread index: 22 \t./data/video/test_ravdess\\fearful\\01-02-06-02-01-02-04.mp4\n",
      "thread index: 23 \t./data/video/test_ravdess\\fearful\\01-02-06-02-02-02-04.mp4\n",
      "thread index: 24 \t./data/video/test_ravdess\\fearful\\02-02-06-01-01-01-01.mp4\n",
      "thread index: 25 \t./data/video/test_ravdess\\fearful\\02-02-06-01-01-02-02.mp4\n",
      "thread index: 26 \t./data/video/test_ravdess\\fearful\\02-02-06-01-02-02-02.mp4\n",
      "thread index: 27 \t./data/video/test_ravdess\\fearful\\02-02-06-02-01-01-02.mp4\n",
      "thread index: 28 \t./data/video/test_ravdess\\fearful\\02-02-06-02-01-01-03.mp4\n",
      "thread index: 29 \t./data/video/test_ravdess\\fearful\\02-02-06-02-02-01-01.mp4\n",
      "thread index: 30 \t./data/video/test_ravdess\\happy\\01-02-03-01-02-01-04.mp4\n",
      "thread index: 31 \t./data/video/test_ravdess\\happy\\01-02-03-01-02-02-04.mp4\n",
      "thread index: 32 \t./data/video/test_ravdess\\happy\\01-02-03-02-02-02-04.mp4\n",
      "thread index: 33 \t./data/video/test_ravdess\\happy\\02-02-03-01-01-01-01.mp4\n",
      "thread index: 34 \t./data/video/test_ravdess\\happy\\02-02-03-01-02-01-03.mp4\n",
      "thread index: 35 \t./data/video/test_ravdess\\happy\\02-02-03-01-02-02-01.mp4\n",
      "thread index: 36 \t./data/video/test_ravdess\\happy\\02-02-03-01-02-02-02.mp4\n",
      "thread index: 37 \t./data/video/test_ravdess\\happy\\02-02-03-02-01-02-01.mp4\n",
      "thread index: 38 \t./data/video/test_ravdess\\happy\\02-02-03-02-02-01-01.mp4\n",
      "thread index: 39 \t./data/video/test_ravdess\\happy\\02-02-03-02-02-01-02.mp4\n",
      "thread index: 40 \t./data/video/test_ravdess\\neutral\\02-02-01-01-01-01-02.mp4\n",
      "thread index: 41 \t./data/video/test_ravdess\\neutral\\02-02-01-01-01-01-04.mp4\n",
      "thread index: 42 \t./data/video/test_ravdess\\neutral\\02-02-01-01-01-02-03.mp4\n",
      "thread index: 43 \t./data/video/test_ravdess\\neutral\\02-02-01-01-02-01-01.mp4\n",
      "thread index: 44 \t./data/video/test_ravdess\\neutral\\02-02-01-01-02-02-01.mp4\n",
      "thread index: 45 \t./data/video/test_ravdess\\sad\\01-02-04-01-02-01-04.mp4\n",
      "thread index: 46 \t./data/video/test_ravdess\\sad\\01-02-04-02-02-01-04.mp4\n",
      "thread index: 47 \t./data/video/test_ravdess\\sad\\02-02-04-01-01-02-03.mp4\n",
      "thread index: 48 \t./data/video/test_ravdess\\sad\\02-02-04-01-02-01-01.mp4\n",
      "thread index: 49 \t./data/video/test_ravdess\\sad\\02-02-04-01-02-02-02.mp4\n",
      "thread index: 50 \t./data/video/test_ravdess\\sad\\02-02-04-01-02-02-03.mp4\n",
      "thread index: 51 \t./data/video/test_ravdess\\sad\\02-02-04-02-01-01-02.mp4\n",
      "thread index: 52 \t./data/video/test_ravdess\\sad\\02-02-04-02-01-02-01.mp4\n",
      "thread index: 53 \t./data/video/test_ravdess\\sad\\02-02-04-02-02-01-02.mp4\n",
      "thread index: 54 \t./data/video/test_ravdess\\sad\\02-02-04-02-02-02-03.mp4\n",
      "all threads is finished\n"
     ]
    }
   ],
   "source": [
    "video_dir_test ='./data/video/test_ravdess'\n",
    "frame_dir_test = './data/frame/test_ravdess'\n",
    "run_vdo2frame(video_dir_test, frame_dir_test, n_thread = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6b7d",
   "metadata": {},
   "source": [
    "## Face localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e5ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame2face(func_path, predictor_path, image_root_folder, save_root_folder, cnn_face_detector, gpu_id=0):\n",
    "\n",
    "    linux_command = 'python {:} {:} {:} {:} {:} {:}'.format(func_path, predictor_path, \n",
    "                                                            image_root_folder, save_root_folder, cnn_face_detector, gpu_id)\n",
    "    print('{:}'.format(image_root_folder))\n",
    "    subprocess.getstatusoutput(linux_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76dd7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_frame2face(frame_dir, face_dir, n_thread):\n",
    "    threads = []\n",
    "    # function\n",
    "    func_path = './data/face_alignment_code/lib/face_align_cuda.py'\n",
    "    # Model\n",
    "    predictor_path      = './data/face_alignment_code/lib/shape_predictor_5_face_landmarks.dat'\n",
    "    cnn_face_detector   = './data/face_alignment_code/lib/mmod_human_face_detector.dat' \n",
    "    for category in os.listdir(frame_dir):\n",
    "        # create category directory\n",
    "        category_dir = os.path.join(frame_dir, category)\n",
    "        for frame_file in os.listdir(category_dir):\n",
    "            # get frame file name and path for each frame\n",
    "            frame_root_folder = os.path.join(category_dir, frame_file)\n",
    "            # create output path for face localization\n",
    "            face_root_folder = frame_root_folder.replace(frame_dir, face_dir)\n",
    "            if os.path.isdir(frame_root_folder):\n",
    "                makefile(face_root_folder)\n",
    "                threads.append(threadFun(frame2face, (func_path, predictor_path, frame_root_folder, face_root_folder, cnn_face_detector)))\n",
    "\n",
    "    run_threads(threads, n_thread)\n",
    "    print('all is over')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2fddc-94c8-4208-8783-34bcb1347500",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d498f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread index: 0 \t./data/frame/train_ravdess\\angry\\01-02-05-01-01-01-04\n",
      "thread index: 1 \t./data/frame/train_ravdess\\angry\\01-02-05-01-01-02-04\n",
      "thread index: 2 \t./data/frame/train_ravdess\\angry\\01-02-05-01-02-01-04\n",
      "thread index: 3 \t./data/frame/train_ravdess\\angry\\01-02-05-01-02-02-04\n",
      "thread index: 4 \t./data/frame/train_ravdess\\angry\\01-02-05-02-02-01-04\n",
      "thread index: 5 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-01-01\n",
      "thread index: 6 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-01-02\n",
      "thread index: 7 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-02-01\n",
      "thread index: 8 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-02-02\n",
      "thread index: 9 \t./data/frame/train_ravdess\\angry\\02-02-05-01-01-02-03\n",
      "thread index: 10 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-01-02\n",
      "thread index: 11 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-01-03\n",
      "thread index: 12 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-02-01\n",
      "thread index: 13 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-02-02\n",
      "thread index: 14 \t./data/frame/train_ravdess\\angry\\02-02-05-01-02-02-03\n",
      "thread index: 15 \t./data/frame/train_ravdess\\angry\\02-02-05-02-01-01-02\n",
      "thread index: 16 \t./data/frame/train_ravdess\\angry\\02-02-05-02-01-01-03\n",
      "thread index: 17 \t./data/frame/train_ravdess\\angry\\02-02-05-02-01-02-01\n",
      "thread index: 18 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-01-03\n",
      "thread index: 19 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-02-01\n",
      "thread index: 20 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-02-02\n",
      "thread index: 21 \t./data/frame/train_ravdess\\angry\\02-02-05-02-02-02-03\n",
      "thread index: 22 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-01-01\n",
      "thread index: 23 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-01-02\n",
      "thread index: 24 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-02-01\n",
      "thread index: 25 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-02-02\n",
      "thread index: 26 \t./data/frame/train_ravdess\\calm\\02-02-02-01-01-02-04\n",
      "thread index: 27 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-01-01\n",
      "thread index: 28 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-01-02\n",
      "thread index: 29 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-01-03\n",
      "thread index: 30 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-02-01\n",
      "thread index: 31 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-02-02\n",
      "thread index: 32 \t./data/frame/train_ravdess\\calm\\02-02-02-01-02-02-03\n",
      "thread index: 33 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-01-02\n",
      "thread index: 34 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-01-03\n",
      "thread index: 35 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-01-04\n",
      "thread index: 36 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-02-01\n",
      "thread index: 37 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-02-02\n",
      "thread index: 38 \t./data/frame/train_ravdess\\calm\\02-02-02-02-01-02-04\n",
      "thread index: 39 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-01-02\n",
      "thread index: 40 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-01-03\n",
      "thread index: 41 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-01-04\n",
      "thread index: 42 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-02-01\n",
      "thread index: 43 \t./data/frame/train_ravdess\\calm\\02-02-02-02-02-02-04\n",
      "thread index: 44 \t./data/frame/train_ravdess\\fearful\\01-02-06-01-01-01-04\n",
      "thread index: 45 \t./data/frame/train_ravdess\\fearful\\01-02-06-01-01-02-04\n",
      "thread index: 46 \t./data/frame/train_ravdess\\fearful\\01-02-06-01-02-02-04\n",
      "thread index: 47 \t./data/frame/train_ravdess\\fearful\\01-02-06-02-02-01-04\n",
      "thread index: 48 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-01-02\n",
      "thread index: 49 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-01-03\n",
      "thread index: 50 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-02-01\n",
      "thread index: 51 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-01-02-03\n",
      "thread index: 52 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-01-01\n",
      "thread index: 53 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-01-02\n",
      "thread index: 54 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-01-03\n",
      "thread index: 55 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-02-01\n",
      "thread index: 56 \t./data/frame/train_ravdess\\fearful\\02-02-06-01-02-02-03\n",
      "thread index: 57 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-01-01\n",
      "thread index: 58 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-02-01\n",
      "thread index: 59 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-02-02\n",
      "thread index: 60 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-01-02-03\n",
      "thread index: 61 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-01-02\n",
      "thread index: 62 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-01-03\n",
      "thread index: 63 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-02-01\n",
      "thread index: 64 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-02-02\n",
      "thread index: 65 \t./data/frame/train_ravdess\\fearful\\02-02-06-02-02-02-03\n",
      "thread index: 66 \t./data/frame/train_ravdess\\happy\\01-02-03-01-01-01-04\n",
      "thread index: 67 \t./data/frame/train_ravdess\\happy\\01-02-03-01-01-02-04\n",
      "thread index: 68 \t./data/frame/train_ravdess\\happy\\01-02-03-02-01-01-04\n",
      "thread index: 69 \t./data/frame/train_ravdess\\happy\\01-02-03-02-01-02-04\n",
      "thread index: 70 \t./data/frame/train_ravdess\\happy\\01-02-03-02-02-01-04\n",
      "thread index: 71 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-01-02\n",
      "thread index: 72 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-01-03\n",
      "thread index: 73 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-02-01\n",
      "thread index: 74 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-02-02\n",
      "thread index: 75 \t./data/frame/train_ravdess\\happy\\02-02-03-01-01-02-03\n",
      "thread index: 76 \t./data/frame/train_ravdess\\happy\\02-02-03-01-02-01-01\n",
      "thread index: 77 \t./data/frame/train_ravdess\\happy\\02-02-03-01-02-01-02\n",
      "thread index: 78 \t./data/frame/train_ravdess\\happy\\02-02-03-01-02-02-03\n",
      "thread index: 79 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-01-01\n",
      "thread index: 80 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-01-02\n",
      "thread index: 81 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-01-03\n",
      "thread index: 82 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-02-02\n",
      "thread index: 83 \t./data/frame/train_ravdess\\happy\\02-02-03-02-01-02-03\n",
      "thread index: 84 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-01-03\n",
      "thread index: 85 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-02-01\n",
      "thread index: 86 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-02-02\n",
      "thread index: 87 \t./data/frame/train_ravdess\\happy\\02-02-03-02-02-02-03\n",
      "thread index: 88 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-01-01\n",
      "thread index: 89 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-01-03\n",
      "thread index: 90 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-02-01\n",
      "thread index: 91 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-02-02\n",
      "thread index: 92 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-01-02-04\n",
      "thread index: 93 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-01-02\n",
      "thread index: 94 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-01-03\n",
      "thread index: 95 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-01-04\n",
      "thread index: 96 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-02-02\n",
      "thread index: 97 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-02-03\n",
      "thread index: 98 \t./data/frame/train_ravdess\\neutral\\02-02-01-01-02-02-04\n",
      "thread index: 99 \t./data/frame/train_ravdess\\sad\\01-02-04-01-01-01-04\n",
      "thread index: 100 \t./data/frame/train_ravdess\\sad\\01-02-04-01-01-02-04\n",
      "thread index: 101 \t./data/frame/train_ravdess\\sad\\01-02-04-01-02-02-04\n",
      "thread index: 102 \t./data/frame/train_ravdess\\sad\\01-02-04-02-01-01-04\n",
      "thread index: 103 \t./data/frame/train_ravdess\\sad\\01-02-04-02-01-02-04\n",
      "thread index: 104 \t./data/frame/train_ravdess\\sad\\01-02-04-02-02-02-04\n",
      "thread index: 105 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-01-01\n",
      "thread index: 106 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-01-02\n",
      "thread index: 107 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-01-03\n",
      "thread index: 108 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-02-01\n",
      "thread index: 109 \t./data/frame/train_ravdess\\sad\\02-02-04-01-01-02-02\n",
      "thread index: 110 \t./data/frame/train_ravdess\\sad\\02-02-04-01-02-01-02\n",
      "thread index: 111 \t./data/frame/train_ravdess\\sad\\02-02-04-01-02-01-03\n",
      "thread index: 112 \t./data/frame/train_ravdess\\sad\\02-02-04-01-02-02-01\n",
      "thread index: 113 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-01-01\n",
      "thread index: 114 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-01-03\n",
      "thread index: 115 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-02-02\n",
      "thread index: 116 \t./data/frame/train_ravdess\\sad\\02-02-04-02-01-02-03\n",
      "thread index: 117 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-01-01\n",
      "thread index: 118 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-01-03\n",
      "thread index: 119 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-02-01\n",
      "thread index: 120 \t./data/frame/train_ravdess\\sad\\02-02-04-02-02-02-02\n",
      "all is over\n"
     ]
    }
   ],
   "source": [
    "frame_dir_train = './data/frame/train_ravdess'\n",
    "face_dir_train  = './data/face/train_ravdess'\n",
    "run_frame2face(frame_dir_train, face_dir_train, n_thread=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afeb268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread index: 0 \t./data/frame/test_ravdess\\angry\\01-02-05-02-01-01-04\n",
      "thread index: 1 \t./data/frame/test_ravdess\\angry\\01-02-05-02-01-02-04\n",
      "thread index: 2 \t./data/frame/test_ravdess\\angry\\01-02-05-02-02-02-04\n",
      "thread index: 3 \t./data/frame/test_ravdess\\angry\\02-02-05-01-01-01-03\n",
      "thread index: 4 \t./data/frame/test_ravdess\\angry\\02-02-05-01-02-01-01\n",
      "thread index: 5 \t./data/frame/test_ravdess\\angry\\02-02-05-02-01-01-01\n",
      "thread index: 6 \t./data/frame/test_ravdess\\angry\\02-02-05-02-01-02-02\n",
      "thread index: 7 \t./data/frame/test_ravdess\\angry\\02-02-05-02-01-02-03\n",
      "thread index: 8 \t./data/frame/test_ravdess\\angry\\02-02-05-02-02-01-01\n",
      "thread index: 9 \t./data/frame/test_ravdess\\angry\\02-02-05-02-02-01-02\n",
      "thread index: 10 \t./data/frame/test_ravdess\\calm\\02-02-02-01-01-01-03\n",
      "thread index: 11 \t./data/frame/test_ravdess\\calm\\02-02-02-01-01-01-04\n",
      "thread index: 12 \t./data/frame/test_ravdess\\calm\\02-02-02-01-01-02-03\n",
      "thread index: 13 \t./data/frame/test_ravdess\\calm\\02-02-02-01-02-01-04\n",
      "thread index: 14 \t./data/frame/test_ravdess\\calm\\02-02-02-01-02-02-04\n",
      "thread index: 15 \t./data/frame/test_ravdess\\calm\\02-02-02-02-01-01-01\n",
      "thread index: 16 \t./data/frame/test_ravdess\\calm\\02-02-02-02-01-02-03\n",
      "thread index: 17 \t./data/frame/test_ravdess\\calm\\02-02-02-02-02-01-01\n",
      "thread index: 18 \t./data/frame/test_ravdess\\calm\\02-02-02-02-02-02-02\n",
      "thread index: 19 \t./data/frame/test_ravdess\\calm\\02-02-02-02-02-02-03\n",
      "thread index: 20 \t./data/frame/test_ravdess\\fearful\\01-02-06-01-02-01-04\n",
      "thread index: 21 \t./data/frame/test_ravdess\\fearful\\01-02-06-02-01-01-04\n",
      "thread index: 22 \t./data/frame/test_ravdess\\fearful\\01-02-06-02-01-02-04\n",
      "thread index: 23 \t./data/frame/test_ravdess\\fearful\\01-02-06-02-02-02-04\n",
      "thread index: 24 \t./data/frame/test_ravdess\\fearful\\02-02-06-01-01-01-01\n",
      "thread index: 25 \t./data/frame/test_ravdess\\fearful\\02-02-06-01-01-02-02\n",
      "thread index: 26 \t./data/frame/test_ravdess\\fearful\\02-02-06-01-02-02-02\n",
      "thread index: 27 \t./data/frame/test_ravdess\\fearful\\02-02-06-02-01-01-02\n",
      "thread index: 28 \t./data/frame/test_ravdess\\fearful\\02-02-06-02-01-01-03\n",
      "thread index: 29 \t./data/frame/test_ravdess\\fearful\\02-02-06-02-02-01-01\n",
      "thread index: 30 \t./data/frame/test_ravdess\\happy\\01-02-03-01-02-01-04\n",
      "thread index: 31 \t./data/frame/test_ravdess\\happy\\01-02-03-01-02-02-04\n",
      "thread index: 32 \t./data/frame/test_ravdess\\happy\\01-02-03-02-02-02-04\n",
      "thread index: 33 \t./data/frame/test_ravdess\\happy\\02-02-03-01-01-01-01\n",
      "thread index: 34 \t./data/frame/test_ravdess\\happy\\02-02-03-01-02-01-03\n",
      "thread index: 35 \t./data/frame/test_ravdess\\happy\\02-02-03-01-02-02-01\n",
      "thread index: 36 \t./data/frame/test_ravdess\\happy\\02-02-03-01-02-02-02\n",
      "thread index: 37 \t./data/frame/test_ravdess\\happy\\02-02-03-02-01-02-01\n",
      "thread index: 38 \t./data/frame/test_ravdess\\happy\\02-02-03-02-02-01-01\n",
      "thread index: 39 \t./data/frame/test_ravdess\\happy\\02-02-03-02-02-01-02\n",
      "thread index: 40 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-01-01-02\n",
      "thread index: 41 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-01-01-04\n",
      "thread index: 42 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-01-02-03\n",
      "thread index: 43 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-02-01-01\n",
      "thread index: 44 \t./data/frame/test_ravdess\\neutral\\02-02-01-01-02-02-01\n",
      "thread index: 45 \t./data/frame/test_ravdess\\sad\\01-02-04-01-02-01-04\n",
      "thread index: 46 \t./data/frame/test_ravdess\\sad\\01-02-04-02-02-01-04\n",
      "thread index: 47 \t./data/frame/test_ravdess\\sad\\02-02-04-01-01-02-03\n",
      "thread index: 48 \t./data/frame/test_ravdess\\sad\\02-02-04-01-02-01-01\n",
      "thread index: 49 \t./data/frame/test_ravdess\\sad\\02-02-04-01-02-02-02\n",
      "thread index: 50 \t./data/frame/test_ravdess\\sad\\02-02-04-01-02-02-03\n",
      "thread index: 51 \t./data/frame/test_ravdess\\sad\\02-02-04-02-01-01-02\n",
      "thread index: 52 \t./data/frame/test_ravdess\\sad\\02-02-04-02-01-02-01\n",
      "thread index: 53 \t./data/frame/test_ravdess\\sad\\02-02-04-02-02-01-02\n",
      "thread index: 54 \t./data/frame/test_ravdess\\sad\\02-02-04-02-02-02-03\n",
      "all is over\n"
     ]
    }
   ],
   "source": [
    "frame_dir_test = './data/frame/test_ravdess' \n",
    "face_dir_test  = './data/face/test_ravdess'\n",
    "run_frame2face(frame_dir_test, face_dir_test, n_thread=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f06db",
   "metadata": {},
   "source": [
    "## Create text file for train list and label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff1987",
   "metadata": {},
   "source": [
    " Example Tree-Structured Data Directory (same as test data) <br> \n",
    "── train_data <br>\n",
    "   ├── Angry <br>\n",
    "   ├── Disgust <br>\n",
    "   ├── Fear <br>\n",
    "   ├── Happy <br>\n",
    "   ├── Neutral <br>\n",
    "   ├── Sad <br>\n",
    "   └── Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1535d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt(dir_path, output_path):\n",
    "    # Open the output file in write mode\n",
    "    with open(output_path, \"w\") as f:\n",
    "    # Walk through the directory\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            # Skip if no files in the directory\n",
    "            if not files:\n",
    "                continue\n",
    "            # Get the parent directory name as the label\n",
    "            label = os.path.basename(os.path.dirname(root))\n",
    "            # Write the directory path (relative to dir_path) and label to the output file\n",
    "            relative_path = os.path.relpath(root, dir_path)\n",
    "            relative_path = relative_path.replace(\"\\\\\", \"/\")\n",
    "            f.write(f\"{relative_path} {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f97a4-cb15-4030-8492-905ff95175fe",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cb95d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "# Define the directory to scan\n",
    "train_dir_path = \"./data/face/train_ravdess\"\n",
    "\n",
    "# Define the output file path\n",
    "train_output_file_path = \"./data/txt/train_ravdess.txt\"\n",
    "\n",
    "get_txt(train_dir_path, train_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a5205-6196-426a-a0d7-530880363e29",
   "metadata": {},
   "source": [
    "### !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926b6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# Define the directory to scan\n",
    "test_dir_path = \"./data/face/test_ravdess\"\n",
    "\n",
    "# Define the output file path\n",
    "test_output_file_path = \"./data/txt/test_ravdess.txt\"\n",
    "\n",
    "get_txt(test_dir_path, test_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ae545",
   "metadata": {},
   "source": [
    "# Modeling !!Before Start all process (run from here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648e9bd",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d71d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2836aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os, sys, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7b3b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## data generator for afew\n",
    "class VideoDataset(data.Dataset):\n",
    "    def __init__(self, video_root, video_list, rectify_label=None, transform=None, csv = False):\n",
    "\n",
    "        self.imgs_first, self.index = load_imgs_total_frame(video_root, video_list, rectify_label)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        return img_first, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "# \n",
    "class TripleImageDataset(data.Dataset):\n",
    "    def __init__(self, video_root, video_list, rectify_label=None, transform=None):\n",
    "\n",
    "        self.imgs_first, self.imgs_second, self.imgs_third, self.index = load_imgs_tsn(video_root, video_list,\n",
    "                                                                                           rectify_label)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        path_second, target_second = self.imgs_second[index]\n",
    "        img_second = Image.open(path_second).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_second = self.transform(img_second)\n",
    "\n",
    "        path_third, target_third = self.imgs_third[index]\n",
    "        img_third = Image.open(path_third).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_third = self.transform(img_third)\n",
    "        return img_first, img_second, img_third, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "def load_imgs_tsn(video_root, video_list, rectify_label):\n",
    "    imgs_first = list()\n",
    "    imgs_second = list()\n",
    "    imgs_third = list()\n",
    "\n",
    "    with open(video_list, 'r') as imf:\n",
    "        index = []\n",
    "        for id, line in enumerate(imf):\n",
    "\n",
    "            video_label = line.strip().split()\n",
    "\n",
    "            video_name = video_label[0]  # name of video\n",
    "            label = rectify_label[video_label[1]]  # label of video\n",
    "\n",
    "            video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "            ###  for sampling triple imgs in the single video_path  ####\n",
    "\n",
    "            img_lists = os.listdir(video_path)\n",
    "            img_lists.sort()  # sort files by ascending\n",
    "            img_count = len(img_lists)  # number of frames in video\n",
    "            num_per_part = int(img_count) // 3\n",
    "\n",
    "            if int(img_count) > 3:\n",
    "                for i in range(img_count):\n",
    "\n",
    "                    random_select_first = random.randint(0, num_per_part)\n",
    "                    random_select_second = random.randint(num_per_part, num_per_part * 2)\n",
    "                    random_select_third = random.randint(2 * num_per_part, len(img_lists) - 1)\n",
    "\n",
    "                    img_path_first = os.path.join(video_path, img_lists[random_select_first])\n",
    "                    img_path_second = os.path.join(video_path, img_lists[random_select_second])\n",
    "                    img_path_third = os.path.join(video_path, img_lists[random_select_third])\n",
    "\n",
    "                    imgs_first.append((img_path_first, label))\n",
    "                    imgs_second.append((img_path_second, label))\n",
    "                    imgs_third.append((img_path_third, label))\n",
    "\n",
    "            else:\n",
    "                for j in range(len(img_lists)):\n",
    "                    img_path_first = os.path.join(video_path, img_lists[j])\n",
    "                    img_path_second = os.path.join(video_path, random.choice(img_lists))\n",
    "                    img_path_third = os.path.join(video_path, random.choice(img_lists))\n",
    "\n",
    "                    imgs_first.append((img_path_first, label))\n",
    "                    imgs_second.append((img_path_second, label))\n",
    "                    imgs_third.append((img_path_third, label))\n",
    "\n",
    "            ###  return video frame index  #####\n",
    "            index.append(np.ones(img_count) * id)  # id: 0 : 379\n",
    "        index = np.concatenate(index, axis=0)\n",
    "        # index = index.astype(int)\n",
    "    return imgs_first, imgs_second, imgs_third, index\n",
    "\n",
    "\n",
    "def load_imgs_total_frame(video_root, video_list, rectify_label):\n",
    "    imgs_first = list()\n",
    "\n",
    "    with open(video_list, 'r') as imf:\n",
    "        index = []\n",
    "        video_names = []\n",
    "        for id, line in enumerate(imf):\n",
    "\n",
    "            video_label = line.strip().split()\n",
    "\n",
    "            video_name = video_label[0]  # name of video\n",
    "            label = rectify_label[video_label[1]]  # label of video\n",
    "\n",
    "            video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "            ###  for sampling triple imgs in the single video_path  ####\n",
    "\n",
    "            img_lists = os.listdir(video_path)\n",
    "            img_lists.sort()  # sort files by ascending\n",
    "            img_count = len(img_lists)  # number of frames in video\n",
    "\n",
    "            for frame in img_lists:\n",
    "                # pdb.set_trace()\n",
    "                imgs_first.append((os.path.join(video_path, frame), label))\n",
    "            ###  return video frame index  #####\n",
    "            video_names.append(video_name)\n",
    "            index.append(np.ones(img_count) * id)\n",
    "        index = np.concatenate(index, axis=0)\n",
    "        # index = index.astype(int)\n",
    "    return imgs_first, index\n",
    "    \n",
    "## data generator for ck_plus\n",
    "class TenFold_VideoDataset(data.Dataset):\n",
    "    def __init__(self, video_root='', video_list='', rectify_label=None, transform=None, fold=1, run_type='train'):\n",
    "        self.imgs_first, self.index = load_imgs_tenfold_totalframe(video_root, video_list, rectify_label, fold, run_type)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        return img_first, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "class TenFold_TripleImageDataset(data.Dataset):\n",
    "    def __init__(self, video_root='', video_list='', rectify_label=None, transform=None, fold=1, run_type='train'):\n",
    "\n",
    "        self.imgs_first, self.imgs_second, self.imgs_third, self.index = load_imgs_tsn_tenfold(video_root,video_list,rectify_label, fold, run_type)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.video_root = video_root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_first, target_first = self.imgs_first[index]\n",
    "        img_first = Image.open(path_first).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_first = self.transform(img_first)\n",
    "\n",
    "        path_second, target_second = self.imgs_second[index]\n",
    "        img_second = Image.open(path_second).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_second = self.transform(img_second)\n",
    "\n",
    "        path_third, target_third = self.imgs_third[index]\n",
    "        img_third = Image.open(path_third).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img_third = self.transform(img_third)\n",
    "\n",
    "        return img_first, img_second, img_third, target_first, self.index[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_first)\n",
    "\n",
    "\n",
    "def load_imgs_tenfold_totalframe(video_root, video_list, rectify_label, fold, run_type):\n",
    "    imgs_first = list()\n",
    "    new_imf = list()\n",
    "\n",
    "    ''' Make ten-fold list '''\n",
    "    with open(video_list, 'r') as imf:\n",
    "        imf = imf.readlines()\n",
    "    if run_type == 'train':\n",
    "        fold_ = list(range(1, 11))\n",
    "        fold_.remove(fold)  # [1,2,3,4,5,6,7,8,9, 10] -> [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "        for i in fold_:\n",
    "            fold_str = str(i) + '-fold'  # 1-fold\n",
    "            for index, item in enumerate(\n",
    "                    imf):  # 0, '1-fold\\t31\\n' in {[0, '1-fold\\t31\\n'], [1, 'S037/006 Happy\\n'], ...}\n",
    "                if fold_str in item:  # 1-fold in '1-fold\\t31\\n'\n",
    "                    for j in range(index + 1, index + int(item.split()[1]) + 1):  # (0 + 1, 0 + 31 + 1 )\n",
    "                        new_imf.append(imf[j])  # imf[2] = 'S042/006 Happy\\n'\n",
    "\n",
    "    if run_type == 'test':\n",
    "        fold_ = fold\n",
    "        fold_str = str(fold_) + '-fold'\n",
    "        for index, item in enumerate(imf):\n",
    "            if fold_str in item:\n",
    "                for j in range(index + 1, index + int(item.split()[1]) + 1):\n",
    "                    new_imf.append(imf[j])\n",
    "\n",
    "    index = []\n",
    "    for id, line in enumerate(new_imf):\n",
    "\n",
    "        video_label = line.strip().split()\n",
    "\n",
    "        video_name = video_label[0]  # name of video\n",
    "        try:\n",
    "            label = rectify_label[video_label[1]]  # label of video\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "        video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "        ###  for sampling triple imgs in the single video_path  ####\n",
    "        img_lists = os.listdir(video_path)\n",
    "        img_lists.sort()  # sort files by ascending\n",
    "        \n",
    "        img_lists = img_lists[ - int(round(len(img_lists))) : ]\n",
    "\n",
    "        img_count = len(img_lists)  # number of frames in video\n",
    "        for frame in img_lists:\n",
    "            imgs_first.append((os.path.join(video_path, frame), label))\n",
    "        ###  return video frame index  #####\n",
    "        index.append(np.ones(img_count) * id)\n",
    "\n",
    "    index = np.concatenate(index, axis=0)\n",
    "    return imgs_first, index\n",
    "\n",
    "def load_imgs_tsn_tenfold(video_root, video_list, rectify_label, fold, run_type):\n",
    "    imgs_first = list()\n",
    "    imgs_second = list()\n",
    "    imgs_third = list()\n",
    "    new_imf = list()\n",
    "    ''' Make ten-fold list '''\n",
    "    with open(video_list, 'r') as imf:\n",
    "        imf = imf.readlines()\n",
    "    if run_type == 'train':\n",
    "        fold_ = list(range(1, 11))\n",
    "        fold_.remove(fold)  # [1,2,3,4,5,6,7,8,9,10] -> [2,3,4,5,6,7,8,9,10]\n",
    "        for i in fold_:\n",
    "            fold_str = str(i) + '-fold'  # 1-fold\n",
    "            for index, item in enumerate(\n",
    "                    imf):  # 0, '1-fold\\t31\\n' in {[0, '1-fold\\t31\\n'], [1, 'S037/006 Happy\\n'], ...}\n",
    "                if fold_str in item:  # 1-fold in '1-fold\\t31\\n'\n",
    "                    for j in range(index + 1, index + int(item.split()[1]) + 1):  # (0 + 1, 0 + 31 + 1 )\n",
    "                        new_imf.append(imf[j])  # imf[2] = 'S042/006 Happy\\n'\n",
    "    if run_type == 'test':\n",
    "        fold_ = fold\n",
    "        fold_str = str(fold_) + '-fold'\n",
    "        for index, item in enumerate(imf):\n",
    "            if fold_str in item:\n",
    "                for j in range(index + 1, index + int(item.split()[1]) + 1):\n",
    "                    new_imf.append(imf[j])\n",
    "    ''' Make triple-image list '''\n",
    "    index = []\n",
    "    for id, line in enumerate(new_imf):\n",
    "        video_label = line.strip().split()\n",
    "        video_name = video_label[0]  # name of video\n",
    "        label = rectify_label[video_label[1]]  # label of video\n",
    "        video_path = os.path.join(video_root, video_name)  # video_path is the path of each video\n",
    "        ###  for sampling triple imgs in the single video_path  ####\n",
    "        img_lists = os.listdir(video_path)\n",
    "        img_lists.sort()  # sort files by ascending\n",
    "        img_lists = img_lists[ - int(round(len(img_lists))):]\n",
    "        img_count = len(img_lists)  # number of frames in video\n",
    "        num_per_part = int(img_count) // 5\n",
    "        if int(img_count) > 5:\n",
    "            for i in range(img_count):\n",
    "                # pdb.set_trace()\n",
    "                random_select_first = random.randint(0, num_per_part)\n",
    "                random_select_second = random.randint(num_per_part, 2 * num_per_part)\n",
    "                random_select_third = random.randint(2 * num_per_part, 3 * num_per_part)\n",
    "\n",
    "                img_path_first = os.path.join(video_path, img_lists[random_select_first])\n",
    "                img_path_second = os.path.join(video_path, img_lists[random_select_second])\n",
    "                img_path_third = os.path.join(video_path, img_lists[random_select_third])\n",
    "\n",
    "                imgs_first.append((img_path_first, label))\n",
    "                imgs_second.append((img_path_second, label))\n",
    "                imgs_third.append((img_path_third, label))\n",
    "\n",
    "        else:\n",
    "            for j in range(len(img_lists)):\n",
    "                img_path_first = os.path.join(video_path, img_lists[j])\n",
    "                img_path_second = os.path.join(video_path, random.choice(img_lists))\n",
    "                img_path_third = os.path.join(video_path, random.choice(img_lists))\n",
    "\n",
    "                imgs_first.append((img_path_first, label))\n",
    "                imgs_second.append((img_path_second, label))\n",
    "                imgs_third.append((img_path_third, label))\n",
    "\n",
    "        ###  return video frame index  #####\n",
    "        index.append(np.ones(img_count) * id)  # id: 0 : 379\n",
    "    index = np.concatenate(index, axis=0)\n",
    "    # index = index.astype(int)\n",
    "    # pdb.set_trace()\n",
    "    return imgs_first, imgs_second, imgs_third, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137e53e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_loader_base(root_train, list_train, batchsize_train, root_eval, list_eval, batchsize_eval, cate2label):\n",
    "    train_dataset = VideoDataset(\n",
    "        video_root=root_train,\n",
    "        video_list=list_train,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]),\n",
    "    )\n",
    "\n",
    "    val_dataset = VideoDataset(\n",
    "        video_root=root_eval,\n",
    "        video_list=list_eval,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.ToTensor()]),\n",
    "        csv=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batchsize_train, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batchsize_eval, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "    return train_loader, val_loader\n",
    "def data_loader_fan(root_train, list_train, batchsize_train, root_eval, list_eval, batchsize_eval, cate2label):\n",
    "\n",
    "    train_dataset = TripleImageDataset(\n",
    "        video_root=root_train,\n",
    "        video_list=list_train,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()]),\n",
    "    )\n",
    "\n",
    "    val_dataset = VideoDataset(\n",
    "        video_root=root_eval,\n",
    "        video_list=list_eval,\n",
    "        rectify_label=cate2label,\n",
    "        transform=transforms.Compose([transforms.Resize(224), transforms.ToTensor()]),\n",
    "        csv=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batchsize_train, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batchsize_eval, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d99fd95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cate2label = {'CK+':{0: 'Happy', 1: 'Angry', 2: 'Disgust', 3: 'Fear', 4: 'Sad', 5: 'Contempt', 6: 'Surprise',\n",
    "                     'Angry': 1,'Disgust': 2,'Fear': 3,'Happy': 0,'Contempt': 5,'Sad': 4,'Surprise': 6},\n",
    "\n",
    "              'AFEW':{0: 'Happy',1: 'Angry',2: 'Disgust',3: 'Fear',4: 'Sad',5: 'Neutral',6: 'Surprise',\n",
    "                  'Angry': 1,'Disgust': 2,'Fear': 3,'Happy': 0,'Neutral': 5,'Sad': 4,'Surprise': 6},\n",
    "\n",
    "              'RAVDESS':{0: 'neutral',1: 'calm',2: 'happy',3: 'sad',4: 'angry',5: 'fearful',6: 'disgust',7: 'surprised',\n",
    "                  'neutral': 0,'calm': 1,'happy': 2,'sad': 3,'angry': 4,'fearful': 5,'disgust': 6,'surprised': 7}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11476058",
   "metadata": {},
   "source": [
    "### Load data !set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfcd7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root directory of train data\n",
    "root_train = './data/face/train_ravdess'\n",
    "# txt file train list\n",
    "list_train = './data/txt/train_ravdess.txt'\n",
    "batchsize_train= 48\n",
    "root_eval = './data/face/test_ravdess'\n",
    "list_eval = './data/txt/test_ravdess.txt'\n",
    "batchsize_eval= 64\n",
    "train_loader, val_loader = data_loader_fan(root_train, list_train, batchsize_train, root_eval,\n",
    "                                               list_eval, batchsize_eval, cate2label['RAVDESS']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f503fd1e-841c-4e6f-8d12-cbf739ec4649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb2929b-97dd-4da3-9de3-ebd1c88d783f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf65ed",
   "metadata": {},
   "source": [
    "## Load pretrain model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aec8d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9056f-2c95-4117-b09e-9d03590ba90e",
   "metadata": {},
   "source": [
    "### model construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316be5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def norm_angle(angle):\n",
    "    norm_angle = sigmoid(10 * (abs(angle) / 0.7853975 - 1))\n",
    "    return norm_angle\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "###''' self-attention; relation-attention '''\n",
    "\n",
    "class ResNet_AT(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, end2end=True, at_type=''):\n",
    "        self.inplanes = 64\n",
    "        self.end2end = end2end\n",
    "        super(ResNet_AT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.alpha = nn.Sequential(nn.Linear(512, 1),\n",
    "                                   nn.Sigmoid())\n",
    "\n",
    "        self.beta = nn.Sequential(nn.Linear(1024, 1),\n",
    "                                  nn.Sigmoid())\n",
    "\n",
    "        self.pred_fc1 = nn.Linear(512, 7)\n",
    "        self.pred_fc2 = nn.Linear(1024, 7)\n",
    "        self.at_type = at_type\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x='', phrase='train', AT_level='first_level',vectors='',vm='',alphas_from1='',index_matrix=''):\n",
    "\n",
    "        vs = []\n",
    "        alphas = []\n",
    "\n",
    "        assert phrase == 'train' or phrase == 'eval'\n",
    "        assert AT_level == 'first_level' or AT_level == 'second_level' or AT_level == 'pred'\n",
    "        if phrase == 'train':\n",
    "            num_pair = 3\n",
    "\n",
    "            for i in range(num_pair):\n",
    "                f = x[:, :, :, :, i]  # x[128,3,224,224]\n",
    "\n",
    "                f = self.conv1(f)\n",
    "                f = self.bn1(f)\n",
    "                f = self.relu(f)\n",
    "                f = self.maxpool(f)\n",
    "\n",
    "                f = self.layer1(f)\n",
    "                f = self.layer2(f)\n",
    "                f = self.layer3(f)\n",
    "                f = self.layer4(f)\n",
    "                f = self.avgpool(f)\n",
    "\n",
    "                f = f.squeeze(3).squeeze(2)  # f[1, 512, 1, 1] ---> f[1, 512]\n",
    "\n",
    "                # MN_MODEL(first Level)\n",
    "                vs.append(f)\n",
    "                alphas.append(self.alpha(self.dropout(f)))\n",
    "\n",
    "            vs_stack = torch.stack(vs, dim=2)\n",
    "            alphas_stack = torch.stack(alphas, dim=2)\n",
    "\n",
    "            if self.at_type == 'self-attention':\n",
    "                vm1 = vs_stack.mul(alphas_stack).sum(2).div(alphas_stack.sum(2))\n",
    "            if self.at_type == 'self_relation-attention':\n",
    "                vm1 = vs_stack.mul(alphas_stack).sum(2).div(alphas_stack.sum(2))\n",
    "                betas = []\n",
    "                for i in range(len(vs)):\n",
    "                    vs[i] = torch.cat([vs[i], vm1], dim=1)\n",
    "                    betas.append(self.beta(self.dropout(vs[i])))\n",
    "\n",
    "                cascadeVs_stack = torch.stack(vs, dim=2)\n",
    "                betas_stack = torch.stack(betas, dim=2)\n",
    "                output = cascadeVs_stack.mul(betas_stack * alphas_stack).sum(2).div((betas_stack * alphas_stack).sum(2))\n",
    "\n",
    "            if self.at_type == 'self-attention':\n",
    "                vm1 = self.dropout(vm1)\n",
    "                pred_score = self.pred_fc1(vm1)\n",
    "\n",
    "            if self.at_type == 'self_relation-attention':\n",
    "                output = self.dropout2(output)\n",
    "                pred_score = self.pred_fc2(output)\n",
    "\n",
    "            return pred_score\n",
    "\n",
    "        if phrase == 'eval':\n",
    "            if AT_level == 'first_level':\n",
    "                f = self.conv1(x)\n",
    "                f = self.bn1(f)\n",
    "                f = self.relu(f)\n",
    "                f = self.maxpool(f)\n",
    "\n",
    "                f = self.layer1(f)\n",
    "                f = self.layer2(f)\n",
    "                f = self.layer3(f)\n",
    "                f = self.layer4(f)\n",
    "                f = self.avgpool(f)\n",
    "\n",
    "                f = f.squeeze(3).squeeze(2)  # f[1, 512, 1, 1] ---> f[1, 512]\n",
    "                # MN_MODEL(first Level)\n",
    "                alphas = self.alpha(self.dropout(f))\n",
    "\n",
    "                return f, alphas\n",
    "\n",
    "            if AT_level == 'second_level':\n",
    "                assert self.at_type == 'self_relation-attention'\n",
    "                vms = index_matrix.permute(1, 0).mm(vm)  # [381, 21783] -> [21783,381] * [381,512] --> [21783, 512]\n",
    "                vs_cate = torch.cat([vectors, vms], dim=1)\n",
    "\n",
    "                betas = self.beta(self.dropout(vs_cate))\n",
    "                ''' keywords: mean_fc ; weight_sourcefc; sum_alpha; weightmean_sourcefc '''\n",
    "                ''' alpha * beta '''\n",
    "                weight_catefc = vs_cate.mul(alphas_from1)  # [21570,512] * [21570,1] --->[21570,512]\n",
    "                alpha_beta = alphas_from1.mul(betas)\n",
    "                sum_alphabetas = index_matrix.mm(alpha_beta)  # [380,21570] * [21570,1] -> [380,1]\n",
    "                weightmean_catefc = index_matrix.mm(weight_catefc).div(sum_alphabetas)\n",
    "\n",
    "                weightmean_catefc = self.dropout2(weightmean_catefc)\n",
    "                pred_score = self.pred_fc2(weightmean_catefc)\n",
    "\n",
    "                return pred_score\n",
    "\n",
    "            if AT_level == 'pred':\n",
    "                if self.at_type == 'self-attention':\n",
    "                    pred_score = self.pred_fc1(self.dropout(vm))\n",
    "\n",
    "                return pred_score\n",
    "\n",
    "''' self-attention; relation-attention '''\n",
    "def resnet18_at(**kwargs):\n",
    "    # Constructs base a ResNet-18 model.\n",
    "    model = ResNet_AT(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5e150-ba41-4c21-9b23-3dfe8b2b52af",
   "metadata": {},
   "source": [
    "### pretrain model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "985f33ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_parameters(_structure, _parameterDir):\n",
    "\n",
    "    checkpoint = torch.load(_parameterDir)\n",
    "    pretrained_state_dict = checkpoint['state_dict']\n",
    "    model_state_dict = _structure.state_dict()\n",
    "\n",
    "    for key in pretrained_state_dict:\n",
    "        if ((key == 'module.fc.weight') | (key == 'module.fc.bias')):\n",
    "\n",
    "            pass\n",
    "        else:\n",
    "            model_state_dict[key.replace('module.', '')] = pretrained_state_dict[key]\n",
    "\n",
    "    _structure.load_state_dict(model_state_dict)\n",
    "    model = torch.nn.DataParallel(_structure).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faccd7-99d7-4c36-b9d2-25b1b57ce921",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdcd5373-3d18-4791-a67c-1fcda019f3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34085130-abd5-4527-b7f6-358f6ac85cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)  # first position is score; second position is pred.\n",
    "    pred = pred.t()  # .t() is T of matrix (256 * 1) -> (1 * 256)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))  # target.view(1,2,2,-1): (256,) -> (1, 2, 2, 64)\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, at_type=''):\n",
    "\n",
    "    if not os.path.exists('./model'):\n",
    "        os.makedirs('./model')\n",
    "\n",
    "    epoch = state['epoch']\n",
    "    save_dir = './model/'+at_type+'_' + str(epoch) + '_' + str(round(float(state['accuracy']), 4))\n",
    "    torch.save(state, save_dir)\n",
    "    print(save_dir)\n",
    "    \n",
    "def time_now():\n",
    "  ISOTIMEFORMAT='%d-%h-%Y-%H-%M-%S'\n",
    "  string = '{:}'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n",
    "  return string\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir, title):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.log_dir = Path(\"{:}\".format(str(log_dir)))\n",
    "        if not self.log_dir.exists(): os.makedirs(str(self.log_dir))\n",
    "        self.title = title\n",
    "        self.log_file = '{:}/{:}_date_{:}.txt'.format(self.log_dir,title, time_now())\n",
    "        self.file_writer = open(self.log_file, 'a')\n",
    "        \n",
    "        \n",
    "    def print(self, string, fprint=True, is_pp=False):\n",
    "        if is_pp: pp.pprint (string)\n",
    "        else:     print(string)\n",
    "        if fprint:\n",
    "          self.file_writer.write('{:}\\n'.format(string))\n",
    "          self.file_writer.flush()\n",
    "            \n",
    "    def write(self, string):\n",
    "        self.file_writer.write('{:}\\n'.format(string))\n",
    "        self.file_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c343a85-0369-4031-887f-9beed6479cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Start the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda90ef-8206-446a-8861-bed93c23d4bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2394b58f-0f55-43a7-aa51-e2bd4e3fd070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d6ff406-59a0-4a78-ba08-4487373b0d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = Logger('./log/','jupyter_rav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5e1b0d-c393-4ab4-99ac-21dcb80ef3d5",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d243be-03fe-4c0f-940d-7063d0fa8112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c91bbc00-fcf0-4373-b057-19ad0a8c4656",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Laptop GPU'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "761428e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "at_type = ['self-attention', 'self_relation-attention']\n",
    "# select attention type\n",
    "at_type = at_type[1]\n",
    "lr = 1e-4\n",
    "epochs = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38f64d43-fc42-4ac4-a766-ddde689fe97b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self_relation-attention'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d1ae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_structure = resnet18_at(at_type=at_type)\n",
    "_parameterDir = './pretrain_model/Resnet18_FER+_pytorch.pth.tar'\n",
    "model = model_parameters(_structure, _parameterDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca5779",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1708a961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.2)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a31ec-2ecf-4385-b203-95ff6860fefd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "869dd67b-c01a-486a-b040-ed12031e6bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch, logger):\n",
    "    print(\"it's on here1\")\n",
    "    losses = AverageMeter()\n",
    "    topframe = AverageMeter()\n",
    "    topVideo = AverageMeter()\n",
    "    print(\"it's on here2\")\n",
    "    # switch to train mode\n",
    "    output_store_fc = []\n",
    "    target_store = []\n",
    "    index_vector = []\n",
    "    \n",
    "    model.train()\n",
    "    for i, (input_first, input_second, input_third, target_first, index) in enumerate(train_loader):\n",
    "        target_var = target_first.to(DEVICE)\n",
    "        input_var = torch.stack([input_first, input_second , input_third], dim=4).to(DEVICE)\n",
    "        # compute output\n",
    "        ''' model & full_model'''\n",
    "        pred_score = model(input_var)\n",
    "        loss = F.cross_entropy(pred_score, target_var)\n",
    "        loss = loss.sum()\n",
    "        #\n",
    "        output_store_fc.append(pred_score)\n",
    "        target_store.append(target_var)\n",
    "        index_vector.append(index)\n",
    "        # measure accuracy and record loss\n",
    "        acc_iter = accuracy(pred_score.data, target_var, topk=(1,))\n",
    "        losses.update(loss.item(), input_var.size(0))\n",
    "        topframe.update(acc_iter[0], input_var.size(0))\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            logger.print('Epoch: [{:3d}][{:3d}/{:3d}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@1 {topframe.val:.3f} ({topframe.avg:.3f})\\t'\n",
    "                .format(\n",
    "                epoch, i, len(train_loader), loss=losses, topframe=topframe))\n",
    "    losses_per_epoch.append(losses.avg)\n",
    "    index_vector = torch.cat(index_vector, dim=0)  # [256] ... [256]  --->  [21570]\n",
    "    index_matrix = []\n",
    "    for i in range(int(max(index_vector)) + 1):\n",
    "        index_matrix.append(index_vector == i)\n",
    "\n",
    "    index_matrix = torch.stack(index_matrix, dim=0).to(DEVICE).float()  # [21570]  --->  [380, 21570]\n",
    "    output_store_fc = torch.cat(output_store_fc, dim=0)  # [256,7] ... [256,7]  --->  [21570, 7]\n",
    "    target_store = torch.cat(target_store, dim=0).float()  # [256] ... [256]  --->  [21570]\n",
    "    pred_matrix_fc = index_matrix.mm(output_store_fc)  # [380,21570] * [21570, 7] = [380,7]\n",
    "    target_vector = index_matrix.mm(target_store.unsqueeze(1)).squeeze(1).div(\n",
    "        index_matrix.sum(1)).long()  # [380,21570] * [21570,1] -> [380,1] / sum([21570,1]) -> [380]\n",
    "\n",
    "    acc_video = accuracy(pred_matrix_fc.cpu(), target_vector.cpu(), topk=(1,))\n",
    "    topVideo.update(acc_video[0], i + 1)\n",
    "    logger.print(' *Acc@Video {topVideo.avg:.3f}   *Acc@Frame {topframe.avg:.3f} '.format(topVideo=topVideo, topframe=topframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a844ce5-a6fe-4995-9d05-6c5c521d2a62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7eb416-bf4e-4206-afc4-8af67edd385b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val(val_loader, model, at_type, logger):\n",
    "    topVideo = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    output_store_fc = []\n",
    "    output_alpha    = []\n",
    "    target_store = []\n",
    "    index_vector = []\n",
    "    with torch.no_grad():\n",
    "        num_classes = 8\n",
    "        class_metrics = {i: {'tp': 0, 'total': 0} for i in range(num_classes)}\n",
    "        class_metrics2 = {i: {'tp': 0, 'fp': 0, 'fn': 0} for i in range(num_classes)}\n",
    "        for i, (input_var, target, index) in enumerate(val_loader):\n",
    "            # compute output\n",
    "            target = target.to(DEVICE)\n",
    "            input_var = input_var.to(DEVICE)\n",
    "            ''' model & full_model'''\n",
    "            f, alphas = model(input_var, phrase = 'eval')\n",
    "\n",
    "            output_store_fc.append(f)\n",
    "            output_alpha.append(alphas)\n",
    "            target_store.append(target)\n",
    "            index_vector.append(index)\n",
    "\n",
    "        index_vector = torch.cat(index_vector, dim=0)  # [256] ... [256]  --->  [21570]\n",
    "        index_matrix = []\n",
    "        for i in range(int(max(index_vector)) + 1):\n",
    "            index_matrix.append(index_vector == i)\n",
    "\n",
    "        index_matrix = torch.stack(index_matrix, dim=0).to(DEVICE).float()  # [21570]  --->  [380, 21570]\n",
    "        output_store_fc = torch.cat(output_store_fc, dim=0)  # [256,7] ... [256,7]  --->  [21570, 7]\n",
    "        output_alpha    = torch.cat(output_alpha, dim=0)     # [256,1] ... [256,1]  --->  [21570, 1]\n",
    "        target_store = torch.cat(target_store, dim=0).float()  # [256] ... [256]  --->  [21570]\n",
    "        ''' keywords: mean_fc ; weight_sourcefc; sum_alpha; weightmean_sourcefc '''\n",
    "        weight_sourcefc = output_store_fc.mul(output_alpha)   #[21570,512] * [21570,1] --->[21570,512]\n",
    "        sum_alpha = index_matrix.mm(output_alpha) # [380,21570] * [21570,1] -> [380,1]\n",
    "        weightmean_sourcefc = index_matrix.mm(weight_sourcefc).div(sum_alpha)\n",
    "        target_vector = index_matrix.mm(target_store.unsqueeze(1)).squeeze(1).div(\n",
    "            index_matrix.sum(1)).long()  # [380,21570] * [21570,1] -> [380,1] / sum([21570,1]) -> [380]\n",
    "        if at_type == 'self-attention':\n",
    "            pred_score = model(vm=weightmean_sourcefc, phrase='eval', AT_level='pred')\n",
    "        if at_type == 'self_relation-attention':\n",
    "            pred_score  = model(vectors=output_store_fc, vm=weightmean_sourcefc, alphas_from1=output_alpha, index_matrix=index_matrix, phrase='eval', AT_level='second_level')\n",
    "        pred = pred_score.argmax(dim=1)\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            class_metrics[i]['tp'] += ((pred == i) & (target_vector == i)).sum().item()\n",
    "            class_metrics[i]['total'] += (target_vector == i).sum().item()\n",
    "\n",
    "        # Compute the accuracy for each class\n",
    "        for i in range(num_classes):\n",
    "            tp = class_metrics[i]['tp']\n",
    "            total = class_metrics[i]['total']\n",
    "            accuracy_c = tp / total if total > 0 else 0\n",
    "            print(f'Class {i}: Accuracy: {accuracy_c:.3f}')\n",
    "        # Update the class metrics\n",
    "        pred = pred_score.argmax(dim=1)\n",
    "        for i in range(num_classes):\n",
    "            class_metrics2[i]['tp'] += ((pred == i) & (target_vector == i)).sum().item()\n",
    "            class_metrics2[i]['fp'] += ((pred == i) & (target_vector != i)).sum().item()\n",
    "            class_metrics2[i]['fn'] += ((pred != i) & (target_vector == i)).sum().item()\n",
    "\n",
    "        # Compute the precision, recall, and F1 score for each class\n",
    "        for i in range(num_classes):\n",
    "            tp = class_metrics2[i]['tp']\n",
    "            fp = class_metrics2[i]['fp']\n",
    "            fn = class_metrics2[i]['fn']\n",
    "            precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "            print(f'Class {i}: Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}')\n",
    "            \n",
    "            \n",
    "        acc_video = accuracy(pred_score.cpu(), target_vector.cpu(), topk=(1,))\n",
    "        topVideo.update(acc_video[0], i + 1)\n",
    "        logger.print(' *Acc@Video {topVideo.avg:.3f} '.format(topVideo=topVideo))\n",
    "        return topVideo.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c16bc-e79a-4e3b-8140-d65bc781285f",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2f1cb67-2295-4bc1-99b2-2c70af1add37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame attention network (fan) afew dataset, learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "logger.print('frame attention network (fan) afew dataset, learning rate: {:}'.format(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c06654e7-a4cc-402d-b57e-5b4ab4f56d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_acc = 0\n",
    "# train(train_loader, model, optimizer, 1, logger)\n",
    "# acc_epoch = val(val_loader, model, at_type, logger)\n",
    "# is_best = acc_epoch > best_acc\n",
    "# if is_best:\n",
    "#     logger.print('better model!')\n",
    "#     best_acc = max(acc_epoch, best_acc)\n",
    "#     save_checkpoint({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'state_dict': model.state_dict(),\n",
    "#             'accuracy': acc_epoch,\n",
    "#         }, at_type=at_type)\n",
    "        \n",
    "# lr_scheduler.step()\n",
    "# logger.print(\"epoch: {:} learning rate:{:}\".format(epoch+1, optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff336425-974e-4371-bec1-2881652a6885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  0][  0/348]\tLoss 1.9471 (1.9471)\tAcc@1 22.917 (22.917)\t\n",
      "Epoch: [  0][200/348]\tLoss 1.4345 (1.7153)\tAcc@1 52.083 (32.618)\t\n",
      " *Acc@Video 52.066   *Acc@Frame 42.331 \n",
      "Class 0: Accuracy: 0.000\n",
      "Class 1: Accuracy: 0.700\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 0.800\n",
      "Class 4: Accuracy: 0.700\n",
      "Class 5: Accuracy: 0.400\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 1: Precision: 0.467, Recall: 0.700, F1: 0.560\n",
      "Class 2: Precision: 0.714, Recall: 1.000, F1: 0.833\n",
      "Class 3: Precision: 0.571, Recall: 0.800, F1: 0.667\n",
      "Class 4: Precision: 0.875, Recall: 0.700, F1: 0.778\n",
      "Class 5: Precision: 1.000, Recall: 0.400, F1: 0.571\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 65.455 \n",
      "better model!\n",
      "./model/self_relation-attention_1_65.4545\n",
      "epoch: 1 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  1][  0/348]\tLoss 1.3870 (1.3870)\tAcc@1 60.417 (60.417)\t\n",
      "Epoch: [  1][200/348]\tLoss 1.0497 (1.2164)\tAcc@1 77.083 (66.812)\t\n",
      " *Acc@Video 80.992   *Acc@Frame 69.929 \n",
      "Class 0: Accuracy: 0.000\n",
      "Class 1: Accuracy: 0.800\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 0.500\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 1: Precision: 0.727, Recall: 0.800, F1: 0.762\n",
      "Class 2: Precision: 0.833, Recall: 1.000, F1: 0.909\n",
      "Class 3: Precision: 0.625, Recall: 1.000, F1: 0.769\n",
      "Class 4: Precision: 0.909, Recall: 1.000, F1: 0.952\n",
      "Class 5: Precision: 1.000, Recall: 0.500, F1: 0.667\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 78.182 \n",
      "better model!\n",
      "./model/self_relation-attention_2_78.1818\n",
      "epoch: 2 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  2][  0/348]\tLoss 1.1429 (1.1429)\tAcc@1 66.667 (66.667)\t\n",
      "Epoch: [  2][200/348]\tLoss 0.8111 (0.9009)\tAcc@1 77.083 (77.871)\t\n",
      " *Acc@Video 85.950   *Acc@Frame 79.478 \n",
      "Class 0: Accuracy: 0.000\n",
      "Class 1: Accuracy: 1.000\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 0.600\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 1: Precision: 0.714, Recall: 1.000, F1: 0.833\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 0.714, Recall: 1.000, F1: 0.833\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 0.857, Recall: 0.600, F1: 0.706\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 83.636 \n",
      "better model!\n",
      "./model/self_relation-attention_3_83.6364\n",
      "epoch: 3 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  3][  0/348]\tLoss 0.7620 (0.7620)\tAcc@1 85.417 (85.417)\t\n",
      "Epoch: [  3][200/348]\tLoss 0.6506 (0.6782)\tAcc@1 87.500 (85.261)\t\n",
      " *Acc@Video 90.909   *Acc@Frame 86.279 \n",
      "Class 0: Accuracy: 0.200\n",
      "Class 1: Accuracy: 1.000\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 0.700\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 1.000, Recall: 0.200, F1: 0.333\n",
      "Class 1: Precision: 0.714, Recall: 1.000, F1: 0.833\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 0.769, Recall: 1.000, F1: 0.870\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 0.700, F1: 0.824\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 87.273 \n",
      "better model!\n",
      "./model/self_relation-attention_4_87.2727\n",
      "epoch: 4 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  4][  0/348]\tLoss 0.5417 (0.5417)\tAcc@1 89.583 (89.583)\t\n",
      "Epoch: [  4][200/348]\tLoss 0.4209 (0.5174)\tAcc@1 89.583 (90.485)\t\n",
      " *Acc@Video 94.215   *Acc@Frame 91.086 \n",
      "Class 0: Accuracy: 0.200\n",
      "Class 1: Accuracy: 1.000\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 0.900\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 1.000, Recall: 0.200, F1: 0.333\n",
      "Class 1: Precision: 0.714, Recall: 1.000, F1: 0.833\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 0.909, Recall: 1.000, F1: 0.952\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 0.900, F1: 0.947\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 90.909 \n",
      "better model!\n",
      "./model/self_relation-attention_5_90.9091\n",
      "epoch: 5 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  5][  0/348]\tLoss 0.4066 (0.4066)\tAcc@1 91.667 (91.667)\t\n",
      "Epoch: [  5][200/348]\tLoss 0.3031 (0.4034)\tAcc@1 97.917 (93.242)\t\n",
      " *Acc@Video 96.694   *Acc@Frame 93.534 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "better model!\n",
      "./model/self_relation-attention_6_94.5455\n",
      "epoch: 6 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  6][  0/348]\tLoss 0.3360 (0.3360)\tAcc@1 97.917 (97.917)\t\n",
      "Epoch: [  6][200/348]\tLoss 0.3279 (0.3248)\tAcc@1 93.750 (94.455)\t\n",
      " *Acc@Video 97.521   *Acc@Frame 94.828 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 7 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  7][  0/348]\tLoss 0.2252 (0.2252)\tAcc@1 93.750 (93.750)\t\n",
      "Epoch: [  7][200/348]\tLoss 0.3288 (0.2622)\tAcc@1 93.750 (95.647)\t\n",
      " *Acc@Video 97.521   *Acc@Frame 95.833 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 8 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  8][  0/348]\tLoss 0.2159 (0.2159)\tAcc@1 97.917 (97.917)\t\n",
      "Epoch: [  8][200/348]\tLoss 0.1443 (0.2217)\tAcc@1 100.000 (96.113)\t\n",
      " *Acc@Video 97.521   *Acc@Frame 96.228 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 9 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [  9][  0/348]\tLoss 0.1935 (0.1935)\tAcc@1 97.917 (97.917)\t\n",
      "Epoch: [  9][200/348]\tLoss 0.2124 (0.1892)\tAcc@1 97.917 (96.517)\t\n",
      " *Acc@Video 97.521   *Acc@Frame 96.618 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 10 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 10][  0/348]\tLoss 0.1888 (0.1888)\tAcc@1 95.833 (95.833)\t\n",
      "Epoch: [ 10][200/348]\tLoss 0.1616 (0.1670)\tAcc@1 95.833 (96.922)\t\n",
      " *Acc@Video 98.347   *Acc@Frame 97.049 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 11 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 11][  0/348]\tLoss 0.1335 (0.1335)\tAcc@1 95.833 (95.833)\t\n",
      "Epoch: [ 11][200/348]\tLoss 0.0731 (0.1479)\tAcc@1 100.000 (97.253)\t\n",
      " *Acc@Video 98.347   *Acc@Frame 97.402 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 12 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 12][  0/348]\tLoss 0.1573 (0.1573)\tAcc@1 95.833 (95.833)\t\n",
      "Epoch: [ 12][200/348]\tLoss 0.0952 (0.1333)\tAcc@1 100.000 (97.440)\t\n",
      " *Acc@Video 98.347   *Acc@Frame 97.540 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 13 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 13][  0/348]\tLoss 0.0982 (0.0982)\tAcc@1 100.000 (100.000)\t\n",
      "Epoch: [ 13][200/348]\tLoss 0.1056 (0.1188)\tAcc@1 100.000 (97.606)\t\n",
      " *Acc@Video 98.347   *Acc@Frame 97.731 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 14 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 14][  0/348]\tLoss 0.0730 (0.0730)\tAcc@1 100.000 (100.000)\t\n",
      "Epoch: [ 14][200/348]\tLoss 0.1701 (0.1103)\tAcc@1 91.667 (97.958)\t\n",
      " *Acc@Video 99.174   *Acc@Frame 98.012 \n",
      "Class 0: Accuracy: 0.800\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.800, Recall: 0.800, F1: 0.800\n",
      "Class 1: Precision: 0.900, Recall: 0.900, F1: 0.900\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 96.364 \n",
      "better model!\n",
      "./model/self_relation-attention_15_96.3636\n",
      "epoch: 15 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 15][  0/348]\tLoss 0.1377 (0.1377)\tAcc@1 97.917 (97.917)\t\n",
      "Epoch: [ 15][200/348]\tLoss 0.1323 (0.0995)\tAcc@1 93.750 (98.010)\t\n",
      " *Acc@Video 99.174   *Acc@Frame 97.989 \n",
      "Class 0: Accuracy: 0.600\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.750, Recall: 0.600, F1: 0.667\n",
      "Class 1: Precision: 0.818, Recall: 0.900, F1: 0.857\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 94.545 \n",
      "epoch: 16 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 16][  0/348]\tLoss 0.1624 (0.1624)\tAcc@1 93.750 (93.750)\t\n",
      "Epoch: [ 16][200/348]\tLoss 0.1002 (0.0909)\tAcc@1 97.917 (98.342)\t\n",
      " *Acc@Video 100.000   *Acc@Frame 98.360 \n",
      "Class 0: Accuracy: 0.800\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.800, Recall: 0.800, F1: 0.800\n",
      "Class 1: Precision: 0.900, Recall: 0.900, F1: 0.900\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 96.364 \n",
      "epoch: 17 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 17][  0/348]\tLoss 0.1067 (0.1067)\tAcc@1 97.917 (97.917)\t\n",
      "Epoch: [ 17][200/348]\tLoss 0.0524 (0.0839)\tAcc@1 100.000 (98.383)\t\n",
      " *Acc@Video 100.000   *Acc@Frame 98.461 \n",
      "Class 0: Accuracy: 0.800\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.800, Recall: 0.800, F1: 0.800\n",
      "Class 1: Precision: 0.900, Recall: 0.900, F1: 0.900\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 96.364 \n",
      "epoch: 18 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 18][  0/348]\tLoss 0.0952 (0.0952)\tAcc@1 97.917 (97.917)\t\n",
      "Epoch: [ 18][200/348]\tLoss 0.1154 (0.0798)\tAcc@1 97.917 (98.611)\t\n",
      " *Acc@Video 100.000   *Acc@Frame 98.545 \n",
      "Class 0: Accuracy: 0.800\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.800, Recall: 0.800, F1: 0.800\n",
      "Class 1: Precision: 0.900, Recall: 0.900, F1: 0.900\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 96.364 \n",
      "epoch: 19 learning rate:0.0001\n",
      "it's on here1\n",
      "it's on here2\n",
      "Epoch: [ 19][  0/348]\tLoss 0.0551 (0.0551)\tAcc@1 100.000 (100.000)\t\n",
      "Epoch: [ 19][200/348]\tLoss 0.0413 (0.0735)\tAcc@1 100.000 (98.580)\t\n",
      " *Acc@Video 100.000   *Acc@Frame 98.545 \n",
      "Class 0: Accuracy: 0.800\n",
      "Class 1: Accuracy: 0.900\n",
      "Class 2: Accuracy: 1.000\n",
      "Class 3: Accuracy: 1.000\n",
      "Class 4: Accuracy: 1.000\n",
      "Class 5: Accuracy: 1.000\n",
      "Class 6: Accuracy: 0.000\n",
      "Class 7: Accuracy: 0.000\n",
      "Class 0: Precision: 0.800, Recall: 0.800, F1: 0.800\n",
      "Class 1: Precision: 0.900, Recall: 0.900, F1: 0.900\n",
      "Class 2: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 3: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 4: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 5: Precision: 1.000, Recall: 1.000, F1: 1.000\n",
      "Class 6: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Class 7: Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      " *Acc@Video 96.364 \n",
      "epoch: 20 learning rate:0.0001\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "losses_per_epoch = []\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, optimizer, epoch, logger)\n",
    "    acc_epoch = val(val_loader, model, at_type, logger)\n",
    "    is_best = acc_epoch > best_acc\n",
    "    if is_best:\n",
    "        logger.print('better model!')\n",
    "        best_acc = max(acc_epoch, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'accuracy': acc_epoch,\n",
    "        }, at_type=at_type)\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    logger.print(\"epoch: {:} learning rate:{:}\".format(epoch+1, optimizer.param_groups[0]['lr']))\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15753d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses_per_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd00c8f-5cbc-4e28-a526-3badd8a2d615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
